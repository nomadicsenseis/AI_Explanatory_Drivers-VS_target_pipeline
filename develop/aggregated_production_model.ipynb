{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702695c0-c665-431b-8d20-00935651397b",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5dd7c-9f78-42ce-b2b2-262a2c861994",
   "metadata": {},
   "source": [
    "In this notebook I create a way of using the aggregated model in the explaianability dashboard against targets. Basically for each day trhough the year it creates different aggregation for the day agains the previous ones, and then, it creates a prediction with the targets model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb94705-a018-4c58-a765-bfba790df0ff",
   "metadata": {},
   "source": [
    "### Instalations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c5b1a9-db05-489c-9c89-4fe7d07fab64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.8/site-packages (5.19.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.8/site-packages (0.4.2)\n",
      "Collecting darts\n",
      "  Using cached darts-0.30.0-py3-none-any.whl.metadata (52 kB)\n",
      "Collecting shap\n",
      "  Using cached shap-0.44.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.4.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting minepy\n",
      "  Using cached minepy-1.2.6-cp38-cp38-linux_x86_64.whl\n",
      "Collecting dcor\n",
      "  Using cached dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting deap\n",
      "  Using cached deap-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from plotly) (23.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.8/site-packages (from s3fs) (1.34.84)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from s3fs) (2024.3.1)\n",
      "Collecting holidays>=0.11.1 (from darts)\n",
      "  Using cached holidays-0.53-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: joblib>=0.16.0 in /opt/conda/lib/python3.8/site-packages (from darts) (1.4.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.8/site-packages (from darts) (3.7.3)\n",
      "Collecting nfoursid>=1.0.0 (from darts)\n",
      "  Using cached nfoursid-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /opt/conda/lib/python3.8/site-packages (from darts) (1.24.4)\n",
      "Collecting pmdarima>=1.8.0 (from darts)\n",
      "  Using cached pmdarima-2.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
      "Collecting pyod>=0.9.5 (from darts)\n",
      "  Using cached pyod-2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.8/site-packages (from darts) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from darts) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from darts) (1.10.1)\n",
      "Collecting statsforecast>=1.4 (from darts)\n",
      "  Downloading statsforecast-1.7.6-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from darts) (0.14.1)\n",
      "Collecting tbats>=1.1.0 (from darts)\n",
      "  Using cached tbats-1.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/conda/lib/python3.8/site-packages (from darts) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from darts) (4.11.0)\n",
      "Collecting xarray>=0.17.0 (from darts)\n",
      "  Using cached xarray-2023.1.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting xgboost>=1.6.0 (from darts)\n",
      "  Using cached xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
      "  Using cached pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tensorboardX>=2.1 (from darts)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotly s3fs darts shap lightgbm minepy dcor deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b37a11-7288-4c36-9fe3-de55c5959c03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# General\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.float_format\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/compat/__init__.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/hashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/missing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/__init__.py:39\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_supported_reso\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     Resolution,\n\u001b[1;32m     42\u001b[0m     get_supported_reso,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     periods_per_second,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnattype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     NaT,\n\u001b[1;32m     50\u001b[0m     NaTType,\n\u001b[1;32m     51\u001b[0m     iNaT,\n\u001b[1;32m     52\u001b[0m     nat_strings,\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timestamps\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.pyx:49\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timezones\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/tz/tz.py:1557\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.__call__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1554\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances\u001b[38;5;241m.\u001b[39mget(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1557\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnocache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rv, tzlocal_classes)\n\u001b[1;32m   1560\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;66;03m# We also cannot store weak references to None, so we\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m         \u001b[38;5;66;03m# will also not store that.\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances[name] \u001b[38;5;241m=\u001b[39m rv\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/tz/tz.py:1655\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.nocache\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tz:\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzoneinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_zonefile_instance\n\u001b[0;32m-> 1655\u001b[0m     tz \u001b[38;5;241m=\u001b[39m \u001b[43mget_zonefile_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tz:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m   1659\u001b[0m         \u001b[38;5;66;03m# name is not a tzstr unless it has at least\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m         \u001b[38;5;66;03m# one offset. For short values of \"name\", an\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m         \u001b[38;5;66;03m# explicit for loop seems to be the fastest way\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m         \u001b[38;5;66;03m# To determine if a string contains a digit\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/zoneinfo/__init__.py:102\u001b[0m, in \u001b[0;36mget_zonefile_instance\u001b[0;34m(new_instance)\u001b[0m\n\u001b[1;32m     99\u001b[0m     zif \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(get_zonefile_instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cached_instance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zif \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     zif \u001b[38;5;241m=\u001b[39m \u001b[43mZoneInfoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetzoneinfofile_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     get_zonefile_instance\u001b[38;5;241m.\u001b[39m_cached_instance \u001b[38;5;241m=\u001b[39m zif\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zif\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/zoneinfo/__init__.py:34\u001b[0m, in \u001b[0;36mZoneInfoFile.__init__\u001b[0;34m(self, zonefile_stream)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zonefile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TarFile\u001b[38;5;241m.\u001b[39mopen(fileobj\u001b[38;5;241m=\u001b[39mzonefile_stream) \u001b[38;5;28;01mas\u001b[39;00m tf:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzones \u001b[38;5;241m=\u001b[39m {zf\u001b[38;5;241m.\u001b[39mname: tzfile(tf\u001b[38;5;241m.\u001b[39mextractfile(zf), filename\u001b[38;5;241m=\u001b[39mzf\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     35\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m zf \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mgetmembers()\n\u001b[1;32m     36\u001b[0m                       \u001b[38;5;28;01mif\u001b[39;00m zf\u001b[38;5;241m.\u001b[39misfile() \u001b[38;5;129;01mand\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m METADATA_FN}\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# deal with links: They'll point to their parent object. Less\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# waste of memory\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         links \u001b[38;5;241m=\u001b[39m {zl\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzones[zl\u001b[38;5;241m.\u001b[39mlinkname]\n\u001b[1;32m     40\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m zl \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mgetmembers() \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                  zl\u001b[38;5;241m.\u001b[39mislnk() \u001b[38;5;129;01mor\u001b[39;00m zl\u001b[38;5;241m.\u001b[39missym()}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/zoneinfo/__init__.py:34\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zonefile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TarFile\u001b[38;5;241m.\u001b[39mopen(fileobj\u001b[38;5;241m=\u001b[39mzonefile_stream) \u001b[38;5;28;01mas\u001b[39;00m tf:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzones \u001b[38;5;241m=\u001b[39m {zf\u001b[38;5;241m.\u001b[39mname: \u001b[43mtzfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m zf \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mgetmembers()\n\u001b[1;32m     36\u001b[0m                       \u001b[38;5;28;01mif\u001b[39;00m zf\u001b[38;5;241m.\u001b[39misfile() \u001b[38;5;129;01mand\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m METADATA_FN}\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# deal with links: They'll point to their parent object. Less\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# waste of memory\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         links \u001b[38;5;241m=\u001b[39m {zl\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzones[zl\u001b[38;5;241m.\u001b[39mlinkname]\n\u001b[1;32m     40\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m zl \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mgetmembers() \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                  zl\u001b[38;5;241m.\u001b[39mislnk() \u001b[38;5;129;01mor\u001b[39;00m zl\u001b[38;5;241m.\u001b[39missym()}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/tz/tz.py:478\u001b[0m, in \u001b[0;36mtzfile.__init__\u001b[0;34m(self, fileobj, filename)\u001b[0m\n\u001b[1;32m    475\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m _nullcontext(fileobj)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m file_stream:\n\u001b[0;32m--> 478\u001b[0m     tzobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_tzfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_tzdata(tzobj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/tz/tz.py:615\u001b[0m, in \u001b[0;36mtzfile._read_tzfile\u001b[0;34m(self, fileobj)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(typecnt):\n\u001b[1;32m    614\u001b[0m     gmtoff, isdst, abbrind \u001b[38;5;241m=\u001b[39m ttinfo[i]\n\u001b[0;32m--> 615\u001b[0m     gmtoff \u001b[38;5;241m=\u001b[39m \u001b[43m_get_supported_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgmtoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     tti \u001b[38;5;241m=\u001b[39m _ttinfo()\n\u001b[1;32m    617\u001b[0m     tti\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m gmtoff\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dateutil/tz/tz.py:1818\u001b[0m, in \u001b[0;36m_get_supported_offset\u001b[0;34m(second_offset)\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dt\u001b[38;5;241m.\u001b[39mreplace(tzinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m EPOCH)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m-> 1818\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_supported_offset\u001b[39m(second_offset):\n\u001b[1;32m   1819\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m second_offset\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import os\n",
    "import numpy as np\n",
    "# import xlsxwriter\n",
    "import datetime\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "from darts.models import LightGBMModel, RandomForest, LinearRegressionModel\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis\n",
    "\n",
    "from darts.explainability.shap_explainer import ShapExplainer\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from darts.models import LinearRegressionModel, LightGBMModel, RandomForest\n",
    "from calendar import month_name as mn\n",
    "import os\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275211c4-ce08-444a-893f-64399cb0de22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Aggregation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa148103-ea0c-4a30-8859-126e75e6ab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use own bucket and prefix\n",
    "S3_BUCKET_NPS = 'iberia-data-lake' # In this case: iberia-data-lake\n",
    "S3_BUCKET_NPS_PREFIX = 'customer/nps_explainability_model' # In this case: sagemaker/sagemaker-template\n",
    "\n",
    "S3_BUCKET_LF = 'ibdata-prod-ew1-s3-customer'\n",
    "S3_BUCKET_LF_PREFIX = 'customer/load_factor_to_s3_nps_model'\n",
    "\n",
    "S3_PATH_READ_NPS = 'customer/nps_surveys/export_historic'\n",
    "S3_PATH_READ_LF = \"customer/load_factor_to_s3_nps_model\"\n",
    "\n",
    "insert_date_ci='2024-06-25'\n",
    "today_date_str='2024-06-25'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240ac6d-fc34-4280-bbe5-d05bd6c34a7a",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b78186-a9ca-4f47-a374-dfc04a201afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ NPS DATA SOURCE\n",
    "# Read df_nps_surveys\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "# READ TODAY DATA (HISTORIC NPS)\n",
    "today_nps_surveys_prefix = f'{S3_PATH_READ_NPS}/insert_date_ci={today_date_str}/'\n",
    "s3_keys = [item.key for item in s3_resource.Bucket(S3_BUCKET_NPS).objects.filter(Prefix=today_nps_surveys_prefix)]\n",
    "preprocess_paths = [f\"s3://{S3_BUCKET_NPS}/{key}\" for key in s3_keys]\n",
    "\n",
    "df_nps_historic = pd.DataFrame()\n",
    "for file in preprocess_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    df_nps_historic = pd.concat([df_nps_historic, df], axis=0)\n",
    "df_nps_historic = df_nps_historic.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cf998ae-800e-4699-928f-0953832a8a43",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_nps_historic=pd.read_csv('historic.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c142b732-c800-45a0-9413-66d1b4dfb10f",
   "metadata": {},
   "source": [
    "    from datetime import datetime, timedelta\n",
    "    # Format dates as strings for S3 prefixes\n",
    "    yesterday_date_str = '2024-03-29'\n",
    "    # READ PREVIOUS NPS DATA (FOR INCREMENTAL)\n",
    "    yesterday_nps_surveys_prefix = f'{S3_PATH_READ_NPS}/insert_date_ci={yesterday_date_str}/'\n",
    "    yesterday_s3_keys = [item.key for item in s3_resource.Bucket(S3_BUCKET_NPS).objects.filter(Prefix=yesterday_nps_surveys_prefix)]\n",
    "    yesterday_preprocess_paths = [f\"s3://{S3_BUCKET_NPS}/{key}\" for key in yesterday_s3_keys]\n",
    "\n",
    "    df_nps_yesterday = pd.DataFrame()\n",
    "    for file in yesterday_preprocess_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        df_nps_yesterday = pd.concat([df_nps_yesterday, df], axis=0)\n",
    "    df_nps_yesterday = df_nps_yesterday.reset_index(drop=True)\n",
    "\n",
    "    df_nps_incremental = pd.merge(df_nps_historic, df_nps_yesterday, how='left', indicator=True, on=df_nps_historic.columns.tolist())\n",
    "    df_nps_incremental = df_nps_incremental[df_nps_incremental['_merge'] == 'left_only']\n",
    "    df_nps_incremental = df_nps_incremental.drop(columns=['_merge'])\n",
    "    df_nps_incremental = df_nps_incremental.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2af0d-fb19-4f91-8a13-03c0109062e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READ LF DATA SOURCE\n",
    "# lf_dir = 's3://ibdata-prod-ew1-s3-customer/customer/load_factor_to_s3_nps_model/'    \n",
    "load_factor_prefix = f's3://{S3_BUCKET_LF}/{S3_PATH_READ_LF}/'\n",
    "\n",
    "# Assume rol for prod\n",
    "sts_client = boto3.client('sts')\n",
    "assumed_role = sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::320714865578:role/ibdata-prod-role-assume-customer-services-from-ibdata-aip-prod\",\n",
    "    RoleSessionName=\"test\"\n",
    ")\n",
    "credentials = assumed_role['Credentials']\n",
    "fs = s3fs.S3FileSystem(key=credentials['AccessKeyId'], secret=credentials['SecretAccessKey'], token=credentials['SessionToken'])\n",
    "\n",
    "# Listall the files\n",
    "load_factor_list = fs.ls(load_factor_prefix)\n",
    "    \n",
    "print(\"userlog: Read historic load_factor data path %s.\", load_factor_prefix)\n",
    "dataframes = []\n",
    "for file_path in load_factor_list:\n",
    "    try:\n",
    "        file_info = fs.info(file_path)\n",
    "        if file_info['Size'] == 0:\n",
    "            print(f\"Skipping empty file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with fs.open(f's3://{file_path}') as f:\n",
    "            df = pd.read_csv(f)\n",
    "            dataframes.append(df)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Caught EmptyDataError for file: {file_path}, skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "if dataframes:\n",
    "    df_lf_historic = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    df_lf_historic = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f3f4c-c6a7-4b51-b7a2-c494afbf75be",
   "metadata": {},
   "source": [
    "## A little preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e1bab-1a13-4a61-96a1-842ca1d9fa75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_1 = (df_nps_historic['operating_airline_code'].isin(['IB', 'YW']))\n",
    "condition_2 = ((df_nps_historic['invitegroup_ib'] != 3) | (df_nps_historic['invitegroup_ib'].isnull()))\n",
    "condition_3 = (df_nps_historic['invitegroup'] == 2)\n",
    "\n",
    "df_nps_historic = df_nps_historic.loc[condition_1 & (condition_2 & condition_3)]\n",
    "\n",
    "df_lf_historic = df_lf_historic.loc[(df_lf_historic['operating_carrier'].isin(['IB', 'YW']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dc13f-32a7-4e2f-b534-9732df2200b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetime_features = ['date_flight_local', 'scheduled_departure_time_local', 'scheduled_arrival_time_local', 'real_departure_time_local',\n",
    "                     'real_arrival_time_local', 'started']\n",
    "columns_to_cross_kpis=['cabin_in_surveyed_flight','haul']\n",
    "columns_ext = ['tier_level', 'language_code', 'seat_no', 'volume_of_bags', 'number_of_child_in_the_booking', 'number_of_infant_in_the_booking',\n",
    "              'number_of_people_in_the_booking', 'country_code', 'customer_journey_origin', 'customer_journey_destination', 'number_of_flights_in_journey',\n",
    "              'order_of_flight_in_journey', 'marketing_airline_code', 'overall_haul', 'weight_category', 'ff_number', 'ticket_num', 'operating_airline_code',\n",
    "               'nps_category', 'nps_100', 'group_age_survey', 'gender'] # invite_group\n",
    "\n",
    "#'bkg_100_booking', \n",
    "touchpoints = ['bkg_200_journey_preparation', 'pfl_100_checkin', 'pfl_200_security', 'pfl_300_lounge',\n",
    "               'pfl_500_boarding', 'ifl_300_cabin', 'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife',\n",
    "               'ifl_400_food_drink', 'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', 'pun_100_punctuality',\n",
    "               'loy_200_loyalty_programme', 'inm_400_issues_response', 'img_310_ease_contact_phone']\n",
    "\n",
    "# ,'img_320_ease_contact_ibplus_mail'\n",
    "survey_fields = ['cla_600_wifi_t_f', 'tvl_journey_reason']\n",
    "\n",
    "df_nps_historic['date_flight_local'] = pd.to_datetime(df_nps_historic['date_flight_local'])\n",
    "df_lf_historic['flight_date_local'] = pd.to_datetime(df_lf_historic['flight_date_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55925964-0b56-4b4f-bb17-4afa8c6c4bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_historic = df_nps_historic[df_nps_historic['date_flight_local'].dt.year >= 2019]\n",
    "df_nps_historic = df_nps_historic[~df_nps_historic['date_flight_local'].dt.year.isin([2020, 2021])]\n",
    "\n",
    "df_lf_historic = df_lf_historic[~df_lf_historic['flight_date_local'].dt.year.isin([2020, 2021])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7699b3-e33f-4046-a48f-d16c26f7e2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delay_features = ['real_departure_time_local', 'scheduled_departure_time_local']\n",
    "for feat in delay_features:\n",
    "    df_nps_historic[feat] = pd.to_datetime(df_nps_historic[feat], format=\"%Y-%m-%d %H:%M:%S\", errors = 'coerce')\n",
    "            \n",
    "df_nps_historic['delay_departure'] = (df_nps_historic['real_departure_time_local'] - df_nps_historic['scheduled_departure_time_local']).dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eae92d-f14c-42f8-bf81-fa33a3da247c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_historic['haul'] = df_nps_historic['haul'].replace('MH', 'SH')\n",
    "#df_nps_historic['cabin_in_surveyed_flight'] = df_nps_historic['cabin_in_surveyed_flight'].replace('Premium Economy', 'Economy')# Load Factor\n",
    "df_lf_historic['load_factor_business'] = df_lf_historic['pax_business'] / df_lf_historic['capacity_business']\n",
    "df_lf_historic['load_factor_premium_ec'] = df_lf_historic['pax_premium_ec'] / df_lf_historic['capacity_premium_ec']\n",
    "df_lf_historic['load_factor_economy'] = df_lf_historic['pax_economy'] / df_lf_historic['capacity_economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0d55c-b6c7-4afc-9a97-aac650e5abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # OTP\n",
    "df_nps_historic['otp15_takeoff'] = (df_nps_historic['delay_departure'] > 15).astype(int)\n",
    "df_nps_historic['otp30_takeoff'] = (df_nps_historic['delay_departure'] > 30).astype(int)\n",
    "df_nps_historic['otp60_takeoff'] = (df_nps_historic['delay_departure'] > 60).astype(int)\n",
    "\n",
    "# Promoter and Detractor columns\n",
    "df_nps_historic[\"promoter_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Promoter\" else 0)\n",
    "df_nps_historic[\"detractor_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Detractor\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136681e-9799-4bbe-a918-8a517244cb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Factor\n",
    "df_lf_historic['load_factor_business'] = df_lf_historic['pax_business'] / df_lf_historic['capacity_business']\n",
    "df_lf_historic['load_factor_premium_ec'] = df_lf_historic['pax_premium_ec'] / df_lf_historic['capacity_premium_ec']\n",
    "df_lf_historic['load_factor_economy'] = df_lf_historic['pax_economy'] / df_lf_historic['capacity_economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c026e03-ca58-48ee-a3d9-0506085096e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_to_load_factor_column = {\n",
    "    'Economy': 'load_factor_economy',\n",
    "    'Business': 'load_factor_business',\n",
    "    'Premium Economy': 'load_factor_premium_ec'\n",
    "}\n",
    "\n",
    "# HISTORIC\n",
    "df_lf_historic.columns = ['date_flight_local' if x=='flight_date_local' else \n",
    "                                'operating_airline_code' if x=='operating_carrier' else\n",
    "                                'surveyed_flight_number' if x=='op_flight_num' else\n",
    "                                x for x in df_lf_historic.columns]\n",
    "\n",
    "df_historic = pd.merge(df_nps_historic, df_lf_historic, \n",
    "                    how='left', \n",
    "                    on=['date_flight_local', 'operating_airline_code', 'surveyed_flight_number', 'haul'])\n",
    "\n",
    "df_historic['load_factor'] = df_historic.apply(lambda row: row[cabin_to_load_factor_column[row['cabin_in_surveyed_flight']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53dd61-558c-4374-8d1a-565251ad67ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic['cabin_in_surveyed_flight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabd71e-eb81-4591-a89f-4043cc8774ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_historic['delay_departure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4a20d-27eb-42bf-af76-3b9ce06a5652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic['cabin_in_surveyed_flight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8a986-8bd5-4d37-82df-5a4b12856c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nps_historic['real_departure_time_local']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511c3f0-9b9d-4f71-913b-5002103696ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic[df_historic['delay_departure']<0]['delay_departure'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de57319-1916-4da2-85bf-02bde668610a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03012f31-387c-4cf3-b588-9fa009ef51e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Condition for dropping rows\n",
    "condition = (df_historic['cabin_in_surveyed_flight'] == 'Premium Economy') & (df_historic['haul'] == 'SH')\n",
    "\n",
    "# Keeping rows that do not meet the condition\n",
    "df_historic = df_historic[~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e3060-e6a8-42d6-b04d-9e15fe88d6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic['respondent_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958c323-76ca-400d-9db0-3272755635ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic = df_historic.drop_duplicates(subset='respondent_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5e78f-0f10-448e-994e-310a60fce708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = df_historic[df_historic['delay'] != df_historic['delay_departure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546ba70-85c5-48ed-b997-7ce063912afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df['date_flight_local']=pd.to_datetime(filtered_df['date_flight_local'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd6c3a-5623-4f8f-be7d-e95660d03dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=filtered_df[filtered_df['date_flight_local'].dt.year>=2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1cf39-2328-4bcd-b8d1-1e455a52ea40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a09649-75c1-4868-a195-aae610b4641c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_historic is your DataFrame and it has been properly imported\n",
    "respondent_ids = [64986539, 64987164, 65097632, 64890118, 64642526]\n",
    "filtered_df = df_historic[df_historic['respondent_id'].isin(respondent_ids)]\n",
    "filtered_df[['respondent_id', 'otp15_takeoff', 'delay_departure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89185e-5a3e-45a4-88b6-51831026e60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check = pd.read_csv('predictions (9).csv')\n",
    "filtered_df = check[check['respondent_id'].isin(respondent_ids)]\n",
    "filtered_df[['respondent_id', 'otp15_takeoff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271de81-03a4-45a5-8b0d-2ef581ac3880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df[['respondent_id', 'otp15_takeoff', 'delay_departure', 'date_flight_local']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c0b26-ca1e-4a02-91d3-d78187dd6cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[col for col in df_historic.columns if 'issues' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a696e9-3e05-4f6a-ace3-16e7058c1154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_variable_correlations(corr_data, target_var):\n",
    "    # Create correlation-specific graphs with otp15_takeoff\n",
    "    # methods = ['pearson', 'spearman', 'kendall']\n",
    "    methods = ['pearson']\n",
    "    for method in methods:\n",
    "        corr = corr_data.corr(method=method)\n",
    "        target_corr = corr[[target_var]].sort_values(by=target_var, ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(y=target_corr.index, x=target_corr[target_var], palette='coolwarm')\n",
    "        plt.title(f'{method.capitalize()} Correlation with {target_var}')\n",
    "        plt.xlabel(f'{method.capitalize()} Correlation Coefficient')\n",
    "        plt.ylabel('Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "def scatter_plot(df, variable, target):# Create the scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df[variable], df[target], alpha=0.6, edgecolors='w', linewidths=0.5)\n",
    "    plt.title(f'{variable} vs {target}')\n",
    "    plt.xlabel(f'{variable}')\n",
    "    plt.ylabel(f'{target}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd6fdf-8eca-4809-81f4-f5a9aebc4689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data = df_historic[['pun_100_punctuality','delay_departure', 'nps_100', 'date_flight_local']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67c34c-28a2-4020-ba82-a84d17c8b43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cabin= \"Economy\"\n",
    "haul= \"SH\"\n",
    "corr_data = df_historic[(df_historic['cabin_in_surveyed_flight']==cabin) & (df_historic['haul']==haul)][['pun_100_punctuality','otp15_takeoff', 'date_flight_local',  'nps_100']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634a532-141a-49ab-885e-2acce00ff108",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data.corr()['otp15_takeoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e33dc-ae89-464c-9d7d-131aa169222d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target= 'delay_departure'\n",
    "variable = 'date_flight_local'\n",
    "\n",
    "corr_data['date_flight_local']=pd.to_datetime(corr_data['date_flight_local'])\n",
    "# plot_variable_correlations(corr_data, 'nps_100')\n",
    "scatter_plot(corr_data, variable, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ca853-ffc0-45e1-a7d7-b58a16d05281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target= 'pun_100_punctuality'\n",
    "variable = 'delay_departure'\n",
    "corr_data['delay_departure'] = corr_data['delay_departure'].clip(lower=0)\n",
    "plot_variable_correlations(corr_data, 'nps_100')\n",
    "scatter_plot(corr_data, variable, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb171d9-8f3f-440c-b2fc-760891dbb374",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03060e08-6308-4b07-815c-a059f639fc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_issues = df_historic[df_historic['inm_050_issues_t_f']=='Yes'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c1ce5-f9b1-4bf1-af21-b3169fc24e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_no_issues = df_historic[df_historic['inm_050_issues_t_f']=='No'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af476615-c342-4d98-b6ae-ae79233c308a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_issues['respondent_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc9d10-2d8a-4a49-837d-79aceb3b562f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_no_issues['respondent_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63504e5-8f3a-4662-960d-b284b82189b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_nps(promoters, detractors, total_responses):\n",
    "    \"\"\"Calcula el Net Promoter Score (NPS).\"\"\"\n",
    "    if total_responses == 0:\n",
    "        return np.nan\n",
    "    return ((promoters - detractors) / total_responses) * 100\n",
    "\n",
    "def calculate_weighted_nps(group_df):\n",
    "    \"\"\"Calcula el NPS ponderado para un grupo de datos.\"\"\"\n",
    "    promoters_weight = group_df.loc[group_df['nps_100'] > 8, 'monthly_weight'].sum()\n",
    "    detractors_weight = group_df.loc[group_df['nps_100'] <= 6, 'monthly_weight'].sum()\n",
    "    total_weight = group_df['monthly_weight'].sum()\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return (promoters_weight - detractors_weight) / total_weight * 100\n",
    "\n",
    "def calculate_satisfaction(df, variable):\n",
    "    \"\"\"Calcula la tasa de satisfacción para una variable dada, utilizando pesos mensuales si están disponibles.\"\"\"\n",
    "    # Comprobar si la columna 'monthly_weight' existe y no está completamente vacía para los datos relevantes\n",
    "    if 'monthly_weight' in df.columns and not df[df[variable].notnull()]['monthly_weight'].isnull().all():\n",
    "        # Suma de los pesos donde la variable es >= 8 y satisface la condición de estar satisfecho\n",
    "        satisfied_weight = df[df[variable] >= 8]['monthly_weight'].sum()\n",
    "        # Suma de todos los pesos donde la variable no es NaN\n",
    "        total_weight = df[df[variable].notnull()]['monthly_weight'].sum()\n",
    "        # Calcula el porcentaje de satisfacción usando los pesos\n",
    "        if total_weight == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_weight / total_weight) * 100\n",
    "    else:\n",
    "        # Contar respuestas satisfechas\n",
    "        satisfied_count = df[df[variable] >= 8].shape[0]\n",
    "        # Contar total de respuestas válidas\n",
    "        total_count = df[variable].notnull().sum()\n",
    "        # Calcula el porcentaje de satisfacción usando conteo\n",
    "        if total_count == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_count / total_count) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_otp(df, n):\n",
    "    \"\"\"Calcula el On-Time Performance (OTP) como el porcentaje de valores igual a 1.\"\"\"\n",
    "    on_time_count = (df[f'otp{n}_takeoff'] == 0).sum()\n",
    "    total_count = df[f'otp{n}_takeoff'].notnull().sum()\n",
    "    return (on_time_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_load_factor(df, pax_column, capacity_column):\n",
    "    \"\"\"Calcula el factor de carga para una cabina específica.\"\"\"\n",
    "    total_pax = df[pax_column].sum()\n",
    "    total_capacity = df[capacity_column].sum()\n",
    "    # Evitar la división por cero\n",
    "    if total_capacity > 0:\n",
    "        return (total_pax / total_capacity) * 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def calculate_metrics_summary(df, start_date, end_date, touchpoints):\n",
    "    # Filtrar por rango de fechas\n",
    "    df_filtered = df[(df['date_flight_local'] >= pd.to_datetime(start_date)) & (df['date_flight_local'] <= pd.to_datetime(end_date))]\n",
    "    \n",
    "    # Mapeo de cabinas a columnas de pax y capacidad\n",
    "    cabin_mapping = {\n",
    "        'Economy': ('pax_economy', 'capacity_economy'),\n",
    "        'Business': ('pax_business', 'capacity_business'),\n",
    "        'Premium Economy': ('pax_premium_ec', 'capacity_premium_ec')\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for (cabin, haul), group_df in df_filtered.groupby(['cabin_in_surveyed_flight', 'haul']):\n",
    "        \n",
    "        print(f'CABIN/HAUL: {cabin}/{haul}')\n",
    "        result = {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'cabin_in_surveyed_flight': cabin,\n",
    "            'haul': haul,\n",
    "            'otp15_takeoff': calculate_otp(group_df, 15),\n",
    "            'otp30_takeoff': calculate_otp(group_df, 30),\n",
    "            'otp60_takeoff': calculate_otp(group_df, 60),\n",
    "            'mean_delay': group_df[group_df['delay_departure']>0]['delay_departure'].mean()\n",
    "        }\n",
    "        \n",
    "        # Calcula el NPS para el grupo\n",
    "        promoters = (group_df['nps_100'] >= 9).sum()\n",
    "        detractors = (group_df['nps_100'] <= 6).sum()\n",
    "        total_responses = group_df['nps_100'].notnull().sum()\n",
    "        result['NPS'] = calculate_nps(promoters, detractors, total_responses) if total_responses else None\n",
    "        \n",
    "        # Calcula el NPS ponderado para el grupo\n",
    "        result['NPS_weighted'] = calculate_weighted_nps(group_df)\n",
    "        \n",
    "        # Satisfacción para cada touchpoint\n",
    "        for tp in touchpoints:\n",
    "            result[f'{tp}_satisfaction'] = calculate_satisfaction(group_df, tp)\n",
    "            \n",
    "        \n",
    "        # Calcula el factor de carga para la cabina\n",
    "        pax_column, capacity_column = cabin_mapping.get(cabin, (None, None))\n",
    "        if pax_column and capacity_column:\n",
    "            result['load_factor'] = calculate_load_factor(group_df, pax_column, capacity_column)\n",
    "        \n",
    "        results_list.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "def generate_date_intervals(start_date, end_date, freq=1):\n",
    "    \"\"\"Genera una lista de tuplas con intervalos de fechas desde start_date hasta end_date.\"\"\"\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    intervals = []\n",
    "    while start_date < end_date:\n",
    "        intervals.append((start_date, start_date))\n",
    "        start_date = start_date + pd.Timedelta(days=freq)\n",
    "    return intervals\n",
    "\n",
    "def calculate_metrics_for_intervals(df, touchpoints, start_date, end_date, freq):\n",
    "    \"\"\"Calcula las métricas para todos los intervalos posibles hasta end_date.\"\"\"\n",
    "    intervals = generate_date_intervals(start_date, end_date, freq)\n",
    "    all_metrics = []\n",
    "\n",
    "    for interval_start, interval_end in intervals:\n",
    "        interval_metrics = calculate_metrics_summary(df, interval_start, interval_end, touchpoints)\n",
    "        print(f\"Interval: {interval_start} to {interval_end}, Data points: {len(interval_metrics)}\")\n",
    "        all_metrics.append(interval_metrics)\n",
    "\n",
    "    \n",
    "    # Concatenar todos los DataFrames de resultados en uno solo\n",
    "    results_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513c586-321f-4213-872c-a60e60e8b56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_weekly = calculate_metrics_for_intervals(df_historic, touchpoints, '01-01-2022', '04-06-2024', 7)\n",
    "# issues_weekly = calculate_metrics_for_intervals(df_issues, touchpoints, '01-01-2022', '04-06-2024', 7)\n",
    "# no_issues_weekly = calculate_metrics_for_intervals(df_no_issues, touchpoints, '01-01-2022', '04-06-2024', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef8c94-ed71-49f3-b24f-8b2114233b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic['delay_departure'] = df_historic['delay_departure'].clip(lower=0)\n",
    "all_daily = calculate_metrics_for_intervals(df_historic, touchpoints, '01-01-2022', '04-06-2024', 1)\n",
    "# issues_daily = calculate_metrics_for_intervals(df_issues, touchpoints, '01-01-2022', '04-06-2024', 1)\n",
    "# no_issues_daily = calculate_metrics_for_intervals(df_no_issues, touchpoints, '01-01-2022', '04-06-2024', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35325b9a-487b-4d7c-9312-bcf4ccbba444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming all_weekly, issues_weekly, no_issues_weekly are already defined\n",
    "dataframes_daily = {\n",
    "    'all_daily': all_daily,\n",
    "    # 'issues_daily': issues_daily,\n",
    "    # 'no_issues_daily': no_issues_daily\n",
    "}\n",
    "\n",
    "# dataframes_weekly = {\n",
    "#     'all_weekly': all_weekly,\n",
    "#     'issues_weekly': issues_weekly,\n",
    "#     'no_issues_weekly': no_issues_weekly\n",
    "# }\n",
    "\n",
    "# Define the function to filter and select columns\n",
    "def filter_and_select(df, cabin, haul):\n",
    "    filtered_df = df[(df['cabin_in_surveyed_flight'] == cabin) & (df['haul'] == haul)]\n",
    "    cols = [col for col in filtered_df.columns if '_satisfaction' in col] + ['otp15_takeoff', 'otp30_takeoff', 'otp60_takeoff', 'mean_delay', 'load_factor', 'NPS_weighted']\n",
    "    return filtered_df[cols]\n",
    "\n",
    "# Dictionary to hold the results\n",
    "daily_result_dict = {key: {} for key in dataframes_daily.keys()}\n",
    "\n",
    "# Loop through each DataFrame and unique combinations\n",
    "for name, df in dataframes_daily.items():\n",
    "    unique_combinations = df[['cabin_in_surveyed_flight', 'haul']].drop_duplicates()\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        result_df = filter_and_select(df, row['cabin_in_surveyed_flight'], row['haul'])\n",
    "        cabin_haul_key = f\"{row['cabin_in_surveyed_flight']}_{row['haul']}\"\n",
    "        daily_result_dict[name][cabin_haul_key] = result_df\n",
    "        \n",
    "# Dictionary to hold the results\n",
    "# weekly_result_dict = {key: {} for key in dataframes_weekly.keys()}\n",
    "\n",
    "# # Loop through each DataFrame and unique combinations\n",
    "# for name, df in dataframes_weekly.items():\n",
    "#     unique_combinations = df[['cabin_in_surveyed_flight', 'haul']].drop_duplicates()\n",
    "#     for _, row in unique_combinations.iterrows():\n",
    "#         result_df = filter_and_select(df, row['cabin_in_surveyed_flight'], row['haul'])\n",
    "#         cabin_haul_key = f\"{row['cabin_in_surveyed_flight']}_{row['haul']}\"\n",
    "#         weekly_result_dict[name][cabin_haul_key] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de686f2a-390e-40ea-822e-f1cb0a1fbea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4911361-c70c-478e-a7ef-6042bf84e82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "\n",
    "def plot_variable_correlations(corr_data, target_var):\n",
    "    # Create correlation-specific graphs with otp15_takeoff\n",
    "    # methods = ['pearson', 'spearman', 'kendall']\n",
    "    methods = ['pearson']\n",
    "    for method in methods:\n",
    "        corr = corr_data.corr(method=method)\n",
    "        target_corr = corr[[target_var]].sort_values(by=target_var, ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(y=target_corr.index, x=target_corr[target_var], palette='coolwarm')\n",
    "        plt.title(f'{method.capitalize()} Correlation with {target_var}')\n",
    "        plt.xlabel(f'{method.capitalize()} Correlation Coefficient')\n",
    "        plt.ylabel('Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def analyze_correlations(corr_data, name):\n",
    "    # Calculate correlations\n",
    "    pearson_corr = corr_data.corr(method='pearson')\n",
    "    spearman_corr = corr_data.corr(method='spearman')\n",
    "    kendall_corr = corr_data.corr(method='kendall')\n",
    "\n",
    "    # Pearson Correlation Matrix visualization\n",
    "    # plt.figure(figsize=(20, 16))\n",
    "    # sns.heatmap(pearson_corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='gray')\n",
    "    # plt.title('Pearson Correlation Matrix for ' + name)\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    # plt.yticks(rotation=0)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Spearman Correlation Matrix visualization\n",
    "#     plt.figure(figsize=(20, 16))\n",
    "#     sns.heatmap(spearman_corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='gray')\n",
    "#     plt.title('Spearman Correlation Matrix for ' + name)\n",
    "#     plt.xticks(rotation=45, ha='right')\n",
    "#     plt.yticks(rotation=0)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Kendall Correlation Matrix visualization\n",
    "#     plt.figure(figsize=(20, 16))\n",
    "#     sns.heatmap(kendall_corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='gray')\n",
    "#     plt.title('Kendall Correlation Matrix for ' + name)\n",
    "#     plt.xticks(rotation=45, ha='right')\n",
    "#     plt.yticks(rotation=0)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    # Call the function to plot correlations with otp15_takeoff\n",
    "    plot_variable_correlations(corr_data, 'NPS_weighted')\n",
    "\n",
    "    \n",
    "    # Check if any column is entirely NaN and fill or drop accordingly\n",
    "    if corr_data.isnull().all().any():\n",
    "        # Option 1: Drop columns that are completely NaN\n",
    "        corr_data = corr_data.dropna(axis=1, how='all')\n",
    "        # Option 2: Fill completely NaN columns with a placeholder if dropping is not desired\n",
    "        # corr_data = corr_data.fillna(value={col: 0 for col in corr_data.columns if corr_data[col].isnull().all()})\n",
    "        \n",
    "    # Impute NaNs\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    corr_data_imputed = pd.DataFrame(imputer.fit_transform(corr_data), columns=corr_data.columns)\n",
    "\n",
    "    # Feature and target separation\n",
    "    X = corr_data_imputed.drop(columns=['NPS_weighted'])\n",
    "    y = corr_data_imputed['NPS_weighted']\n",
    "\n",
    "    # Calculate Mutual Information\n",
    "    mi = mutual_info_regression(X, y, random_state=42)\n",
    "    mi_df = pd.DataFrame(mi, index=X.columns, columns=['Mutual Information']).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "    # Mutual Information visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=mi_df['Mutual Information'], y=mi_df.index, palette='viridis')\n",
    "    plt.title('Mutual Information between NPS_weighted and Other Variables for ' + name)\n",
    "    plt.xlabel('Mutual Information')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to an Excel file\n",
    "    with pd.ExcelWriter('correlation_analysis_results_' + name + '.xlsx') as writer:\n",
    "        pearson_corr.to_excel(writer, sheet_name='Pearson Correlation')\n",
    "        spearman_corr.to_excel(writer, sheet_name='Spearman Correlation')\n",
    "        kendall_corr.to_excel(writer, sheet_name='Kendall Correlation')\n",
    "        mi_df.to_excel(writer, sheet_name='Mutual Information')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def scatter_plot_with_regression(df, variable, target):\n",
    "    # Remove rows with missing values\n",
    "    df_clean = df.dropna(subset=[variable, target])\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(10, 10))  # Square plot\n",
    "    plt.scatter(df_clean[variable], df_clean[target], alpha=0.6, edgecolors='w', linewidths=0.5, label='Data points')\n",
    "    \n",
    "    # Fit the linear regression model\n",
    "    X = df_clean[variable].values.reshape(-1, 1)\n",
    "    y = df_clean[target].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot the regression line\n",
    "    plt.plot(df_clean[variable], model.predict(X), color='red', label='Fitted line')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(f'{variable} vs {target}')\n",
    "    plt.xlabel(f'{variable}')\n",
    "    plt.ylabel(f'{target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Calculate R^2\n",
    "    r_squared = model.score(X, y)\n",
    "    \n",
    "    # Add slope and R^2 to the plot\n",
    "    plt.text(0.05, 0.10, f'Slope: {model.coef_[0]:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.text(0.05, 0.05, f'R^2: {r_squared:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return model.coef_[0], model.intercept_, r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234c4d1-1f68-4a4c-bafc-d8e062e7eec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scatter_plot(df, variable, target):# Create the scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df[variable], df[target], alpha=0.6, edgecolors='w', linewidths=0.5)\n",
    "    plt.title(f'{variable} vs {target}')\n",
    "    plt.xlabel(f'{variable}')\n",
    "    plt.ylabel(f'{target}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940d075-7204-4dd8-9477-055c801754c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the analysis function to each DataFrame in the result_dict\n",
    "for name, dfs in daily_result_dict.items():\n",
    "    print(name)\n",
    "    for cabin_haul_key, df in dfs.items():\n",
    "        print(cabin_haul_key)\n",
    "        if not df.empty:\n",
    "            analyze_correlations(df, name + '_' + cabin_haul_key)\n",
    "            scatter_plot(df, 'ifl_100_cabin_crew_satisfaction', 'otp15_takeoff')\n",
    "            scatter_plot(df, 'con_100_connections_satisfaction', 'otp15_takeoff')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba2b38-a5e1-48c7-ac08-0e842164dece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0bb96-b340-439d-b925-bb3323eab940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the analysis function to each DataFrame in the result_dict\n",
    "for name, dfs in weekly_result_dict.items():\n",
    "    print(name)\n",
    "    for cabin_haul_key, df in dfs.items():\n",
    "        print(cabin_haul_key)\n",
    "        if not df.empty:\n",
    "            analyze_correlations(df, name + '_' + cabin_haul_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5b1ac-ee5a-4af9-9928-a18f582d5aca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agregation logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313964d3-0e4e-4b79-a22c-ee64a1bb69bd",
   "metadata": {},
   "source": [
    "Given a date it takes it as an \"end_date\" and computes every interval with previous dates. Then it perfomr the satisfaction, NPS, load factor and otp aggregations for that particular interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221c075-f251-4932-b44c-f03a9fb7ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "import numpy as np\n",
    "\n",
    "def calculate_nps(promoters, detractors, total_responses):\n",
    "    \"\"\"Calcula el Net Promoter Score (NPS).\"\"\"\n",
    "    if total_responses == 0:\n",
    "        return np.nan\n",
    "    return ((promoters - detractors) / total_responses) * 100\n",
    "\n",
    "def calculate_weighted_nps(group_df):\n",
    "    \"\"\"Calcula el NPS ponderado para un grupo de datos.\"\"\"\n",
    "    promoters_weight = group_df.loc[group_df['nps_100'] > 8, 'monthly_weight'].sum()\n",
    "    detractors_weight = group_df.loc[group_df['nps_100'] <= 6, 'monthly_weight'].sum()\n",
    "    total_weight = group_df['monthly_weight'].sum()\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return (promoters_weight - detractors_weight) / total_weight * 100\n",
    "\n",
    "def calculate_satisfaction(df, variable):\n",
    "    \"\"\"Calcula la tasa de satisfacción para una variable dada, utilizando pesos mensuales si están disponibles.\"\"\"\n",
    "    # Comprobar si la columna 'monthly_weight' existe y no está completamente vacía para los datos relevantes\n",
    "    if 'monthly_weight' in df.columns and not df[df[variable].notnull()]['monthly_weight'].isnull().all():\n",
    "        # Suma de los pesos donde la variable es >= 8 y satisface la condición de estar satisfecho\n",
    "        satisfied_weight = df[df[variable] >= 8]['monthly_weight'].sum()\n",
    "        # Suma de todos los pesos donde la variable no es NaN\n",
    "        total_weight = df[df[variable].notnull()]['monthly_weight'].sum()\n",
    "        # Calcula el porcentaje de satisfacción usando los pesos\n",
    "        if total_weight == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_weight / total_weight) * 100\n",
    "    else:\n",
    "        # Contar respuestas satisfechas\n",
    "        satisfied_count = df[df[variable] >= 8].shape[0]\n",
    "        # Contar total de respuestas válidas\n",
    "        total_count = df[variable].notnull().sum()\n",
    "        # Calcula el porcentaje de satisfacción usando conteo\n",
    "        if total_count == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_count / total_count) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_otp(df, variable='otp15_takeoff'):\n",
    "    \"\"\"Calcula el On-Time Performance (OTP) como el porcentaje de valores igual a 1.\"\"\"\n",
    "    on_time_count = (df[variable] == 0).sum()\n",
    "    total_count = df[variable].notnull().sum()\n",
    "    return (on_time_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_load_factor(df, pax_column, capacity_column):\n",
    "    \"\"\"Calcula el factor de carga para una cabina específica.\"\"\"\n",
    "    total_pax = df[pax_column].sum()\n",
    "    total_capacity = df[capacity_column].sum()\n",
    "    # Evitar la división por cero\n",
    "    if total_capacity > 0:\n",
    "        return (total_pax / total_capacity) * 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def calculate_metrics_summary(df, start_date, end_date, touchpoints):\n",
    "    # Filtrar por rango de fechas\n",
    "    df_filtered = df[(df['date_flight_local'] >= pd.to_datetime(start_date)) & (df['date_flight_local'] <= pd.to_datetime(end_date))]\n",
    "    \n",
    "    # Mapeo de cabinas a columnas de pax y capacidad\n",
    "    cabin_mapping = {\n",
    "        'Economy': ('pax_economy', 'capacity_economy'),\n",
    "        'Business': ('pax_business', 'capacity_business'),\n",
    "        'Premium Economy': ('pax_premium_ec', 'capacity_premium_ec')\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for (cabin, haul), group_df in df_filtered.groupby(['cabin_in_surveyed_flight', 'haul']):\n",
    "        \n",
    "        print(f'CABIN/HAUL: {cabin}/{haul}')\n",
    "        result = {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'cabin_in_surveyed_flight': cabin,\n",
    "            'haul': haul,\n",
    "            'otp15_takeoff': calculate_otp(group_df)\n",
    "        }\n",
    "        \n",
    "        # Calcula el NPS para el grupo\n",
    "        promoters = (group_df['nps_100'] >= 9).sum()\n",
    "        detractors = (group_df['nps_100'] <= 6).sum()\n",
    "        total_responses = group_df['nps_100'].notnull().sum()\n",
    "        result['NPS'] = calculate_nps(promoters, detractors, total_responses) if total_responses else None\n",
    "        \n",
    "        # Calcula el NPS ponderado para el grupo\n",
    "        result['NPS_weighted'] = calculate_weighted_nps(group_df)\n",
    "        \n",
    "        # Satisfacción para cada touchpoint\n",
    "        for tp in touchpoints:\n",
    "            result[f'{tp}_satisfaction'] = calculate_satisfaction(group_df, tp)\n",
    "            \n",
    "        \n",
    "        # Calcula el factor de carga para la cabina\n",
    "        pax_column, capacity_column = cabin_mapping.get(cabin, (None, None))\n",
    "        if pax_column and capacity_column:\n",
    "            result['load_factor'] = calculate_load_factor(group_df, pax_column, capacity_column)\n",
    "        \n",
    "        results_list.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "def generate_date_intervals(start_date, end_date):\n",
    "    \"\"\"Genera una lista de tuplas con intervalos de fechas desde start_date hasta end_date.\"\"\"\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    intervals = [(start_date + pd.Timedelta(days=d), end_date) for d in range((end_date - start_date).days + 1)]\n",
    "    return intervals\n",
    "\n",
    "def calculate_metrics_for_intervals(df, touchpoints, start_date, end_date):\n",
    "    \"\"\"Calcula las métricas para todos los intervalos posibles hasta end_date.\"\"\"\n",
    "    intervals = generate_date_intervals(start_date, end_date)\n",
    "    all_metrics = []\n",
    "\n",
    "    for interval_start, interval_end in intervals:\n",
    "        interval_metrics = calculate_metrics_summary(df, interval_start, interval_end, touchpoints)\n",
    "        print(f\"Interval: {interval_start} to {interval_end}, Data points: {len(interval_metrics)}\")\n",
    "        all_metrics.append(interval_metrics)\n",
    "\n",
    "    \n",
    "    # Concatenar todos los DataFrames de resultados en uno solo\n",
    "    results_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    return results_df\n",
    "\n",
    "# # Ejemplo de uso:\n",
    "# # touchpoints = ['tp1', 'tp2', 'tp3']  # Asegúrate de reemplazar estos con los nombres reales de tus touchpoints\n",
    "# df_result = calculate_metrics_summary(df_historic, '2023-01-01', '2023-01-31', touchpoints)\n",
    "# # print(df_result)\n",
    "\n",
    "# # Definir la fecha de inicio del año y la fecha de fin específica\n",
    "# start_date = '2023-03-01'\n",
    "# end_date = '2023-05-01'\n",
    "\n",
    "\n",
    "\n",
    "# results_intervals_df = calculate_metrics_for_intervals(df_historic, touchpoints, start_date, end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41884e56-b957-4c27-a1a2-76d033da588c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aplicar la función a cada día desde 2023-01-01 en adelante\n",
    "start_date = '2023-01-01'\n",
    "end_date = df_historic['date_flight_local'].max().strftime('%Y-%m-%d')\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "touchpoints = ['bkg_200_journey_preparation', 'pfl_100_checkin', 'pfl_200_security', 'pfl_300_lounge',\n",
    "               'pfl_500_boarding', 'ifl_300_cabin', 'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife',\n",
    "               'ifl_400_food_drink', 'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', 'pun_100_punctuality',\n",
    "               'loy_200_loyalty_programme', 'inm_400_issues_response', 'img_310_ease_contact_phone']\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for current_date in date_range:\n",
    "    daily_results = calculate_metrics_summary(df_historic, current_date, current_date, touchpoints)\n",
    "    all_results.append(daily_results)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "results_df = pd.concat(all_results, ignore_index=True)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ae21d-9aa5-44d2-9cfc-457ebf8f0b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('daily_aggregation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716509d8-80f0-44a2-8146-cf0fd9b213bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cabin='Economy'\n",
    "haul = 'LH'\n",
    "corr_data = pd.read_csv('daily_aggregation.csv')\n",
    "corr_data = corr_data[corr_data['start_date']>='2023-01-01']\n",
    "variables = [\n",
    "    'pun_100_punctuality_satisfaction',\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\",\n",
    "    \"NPS_weighted\"\n",
    "]\n",
    "\n",
    "corr_data = corr_data[(corr_data['cabin_in_surveyed_flight']==cabin) & (corr_data['haul']==haul)][variables]\n",
    "\n",
    "# Apply the analysis function to each DataFrame in the result_dict\n",
    "analyze_correlations(corr_data, cabin + '_' + haul)\n",
    "\n",
    "scatter_plot_with_regression(corr_data, 'otp15_takeoff', 'arr_100_arrivals_satisfaction')\n",
    "\n",
    "corr_data[['otp15_takeoff','pun_100_punctuality_satisfaction','arr_100_arrivals_satisfaction','NPS_weighted']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244e2e3-b265-40da-9cfd-ae0f9cf94985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "06cc9c26-d30e-4a78-8d26-fc85355748ea",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Assume df_historic and touchpoints are defined elsewhere\n",
    "\n",
    "# Convert start_date and end_date strings to datetime objects for manipulation\n",
    "start_date = datetime.strptime('2024-01-01', '%Y-%m-%d')\n",
    "original_end_date = datetime.strptime('2024-04-08', '%Y-%m-%d')\n",
    "\n",
    "# Initialize an empty DataFrame to store the results from each interval\n",
    "all_intervals_results = pd.DataFrame()\n",
    "\n",
    "# Loop over the range from (original_end_date - 15 days) to original_end_date\n",
    "for offset in range(0, 16):  # Including the 15th day\n",
    "    # Calculate the new end_date for this iteration\n",
    "    end_date = original_end_date - timedelta(days=offset)\n",
    "    \n",
    "    # Convert end_date back to string format if your function expects a string\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Call your function with the current interval's end_date\n",
    "    interval_results = calculate_metrics_for_intervals(df_historic, touchpoints, start_date.strftime('%Y-%m-%d'), end_date_str)\n",
    "    \n",
    "    # Assuming interval_results is a DataFrame, you may want to add a column to indicate the end_date for this interval's results\n",
    "    interval_results['interval_end_date'] = end_date_str\n",
    "    \n",
    "    # Append the results for this interval to the all_intervals_results DataFrame\n",
    "    all_intervals_results = pd.concat([all_intervals_results, interval_results])\n",
    "\n",
    "# Reset the index of the final DataFrame if necessary\n",
    "all_intervals_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Now, all_intervals_results contains the metrics calculated for each interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a79f7e-d917-4ac8-82bd-d594d2d138b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_intervals_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048a215-7a10-40e3-a572-9d93297f1ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_intervals_results.to_csv('intervals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78e6c9-aa27-4e50-b80b-1f4e7c5cad94",
   "metadata": {},
   "source": [
    "# 2. Prediction with Darts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcf1e2-b108-4246-ba5b-2e9811536db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1920a-ffad-4f37-86ae-67c2947928e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_historic[(df_historic['cabin_in_surveyed_flight']=='Economy') & (df_historic['haul']=='SH')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276ad55-96b9-40e2-aa22-2c74bf87acb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_issues_BLH = df_issues[(df_issues['cabin_in_surveyed_flight']=='Economy') & (df_issues['haul']=='SH')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7443e-9b51-40da-a39d-178beb3292ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_no_issues_BLH = df_no_issues[(df_no_issues['cabin_in_surveyed_flight']=='Economy') & (df_no_issues['haul']=='SH')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a612f-0fde-440b-b966-8898c8ab0a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88317a99-d209-499e-8c3b-051d187442fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_agg = calculate_metrics_summary(df_all, '2024-01-01', '2024-05-24', touchpoints)\n",
    "issues_BLH = calculate_metrics_summary(df_issues_BLH, '2024-01-01', '2024-05-24', touchpoints)\n",
    "no_issues_BLH = calculate_metrics_summary(df_no_issues_BLH, '2024-01-01', '2024-05-24', touchpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc30b0-7d14-4882-9fcb-2352341a9557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "issues_BLH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b591c6-3f48-4393-a90f-7a9025248103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the DataFrames\n",
    "concatenated_df = pd.concat([issues_BLH, no_issues_BLH], ignore_index=True)\n",
    "concatenated_df['insert_date_ci']='2024-06-01'\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfd0d4-b7f1-449d-aa5d-4f0708b3ed35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# List of libraries you want to check versions for\n",
    "libraries = [\n",
    "    \"s3fs\", \"boto\", \"boto3\", \"botocore\", \"numpy\", \"scikit-image\",\n",
    "    \"scikit-learn\", \"scipy\", \"PyYAML\", \"pandas\", \"darts\",\n",
    "    \"optuna\", \"shap\", \"lightgbm\"\n",
    "]\n",
    "\n",
    "# Check the installed version for each library and print it\n",
    "for library in libraries:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(library).version\n",
    "        print(f\"{library}: {version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{library} is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16492a-617a-44fc-a62f-28c80ff0b327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "from darts.models import LightGBMModel, RandomForest, LinearRegressionModel\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis\n",
    "\n",
    "from darts.explainability.shap_explainer import ShapExplainer\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from darts.models import LinearRegressionModel, LightGBMModel, RandomForest\n",
    "from calendar import month_name as mn\n",
    "import os\n",
    "\n",
    "import shap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4cc99-6dd3-4450-af4f-e19b76ca7a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fe899-384e-4288-bd71-aa9f8d137fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df = pd.read_csv('operative_performance_corrected_annual_targets_2023-11-21.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec4f80-85fb-43f3-906f-9023d3dd490f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df.rename(columns={\n",
    "    'cabin' : 'cabin_in_surveyed_flight',\n",
    "    'date_flight_local': 'start_date'  # Assuming you want to consider end_date as the equivalent of date_flight_local\n",
    "}, inplace=True)\n",
    "\n",
    "# Correct the conversion to datetime objects\n",
    "year_targets_df['start_date'] = pd.to_datetime(year_targets_df['start_date'])\n",
    "\n",
    "# Compute 'start_date' as the first day of the corresponding month\n",
    "# Using dt.to_period('M').to_timestamp() to safely navigate datetime formats\n",
    "year_targets_df['end_date'] = year_targets_df['start_date'] + pd.offsets.YearEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849bb70-16dc-4d38-afc6-239debd2e2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "satisfaction_cols = [col for col in year_targets_df.columns if col.endswith('_satisfaction')]\n",
    "otp_cols = ['otp15_takeoff']\n",
    "features_cols = satisfaction_cols + ['load_factor'] + otp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a806fa3-5ee4-4b25-bf50-ab73fa4162ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df['insert_date_ci']='2023-11-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6363cb-fb90-473e-97f8-47a4f8b2cd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df=year_targets_df[['start_date', 'end_date','cabin_in_surveyed_flight','haul','otp15_takeoff', 'NPS', 'NPS_weighted']+ satisfaction_cols + ['load_factor','insert_date_ci']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34b740-3304-437c-9574-f5be83c1c2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df= year_targets_df[(year_targets_df['start_date']=='2023-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e478d7-7b52-493a-bef2-dcd09568b2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb65773-f032-4714-8bc0-40f910bcffa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a copy of the first two rows\n",
    "copy_df = year_targets_df[(year_targets_df['haul']=='SH')].copy()\n",
    "\n",
    "# Add 0.5 to the 'otp15_takeoff' column in the copied DataFrame\n",
    "copy_df['otp15_takeoff'] += 0.5\n",
    "\n",
    "copy_df['insert_date_ci'] = '2024-05-08'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078e8bd-a48e-4a3a-8a39-bd31e4a12736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data=pd.read_csv('data_for_historic_prediction (5).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ca151-a410-459e-8b05-48645b36ba2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data['start_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0f043-6ae2-4b2c-a44c-dde566c58da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "satisfaction_cols = [col for col in corr_data.columns if col.endswith('_satisfaction')]\n",
    "otp_cols = ['otp15_takeoff']\n",
    "features_cols = satisfaction_cols + ['load_factor'] + otp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5ee87-c195-4f4c-83fc-2b05afed5017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buss_corr = corr_data[(corr_data['cabin_in_surveyed_flight'] == 'Business') & (corr_data['haul'] == 'SH')][features_cols].corr()\n",
    "eco_corr = corr_data[(corr_data['cabin_in_surveyed_flight'] == 'Economy') & (corr_data['haul'] == 'SH')][features_cols].corr()\n",
    "\n",
    "\n",
    "# Splitting the DataFrame\n",
    "df_business = year_targets_df[(year_targets_df['cabin_in_surveyed_flight'] == 'Business') & (year_targets_df['haul']=='SH')].copy()\n",
    "df_economy = year_targets_df[(year_targets_df['cabin_in_surveyed_flight'] == 'Economy') & (year_targets_df['haul']=='SH')].copy()\n",
    "\n",
    "# Function to adjust values based on the top 5 correlated features and correlation matrix\n",
    "def adjust_top_5_values(df, correlation_matrix, delta_change):\n",
    "    # Identify the top 5 features based on absolute correlation with 'otp15_takeoff'\n",
    "    top_features = correlation_matrix['otp15_takeoff'].abs().nlargest(5).index\n",
    "\n",
    "    # Apply adjustments only to the top 5 features\n",
    "    for column in top_features:\n",
    "        if column != 'otp15_takeoff':  # Ensure we're not adjusting 'otp15_takeoff' again\n",
    "            adjustment_factor = correlation_matrix.at[column, 'otp15_takeoff'] * delta_change\n",
    "            print(f\"Adjustment_factor for column {column}: {adjustment_factor}\")\n",
    "            df[column] += adjustment_factor\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sample usage with your existing DataFrame splits\n",
    "delta_otp15_takeoff = 0.5\n",
    "df_business['otp15_takeoff'] += delta_otp15_takeoff\n",
    "df_economy['otp15_takeoff'] += delta_otp15_takeoff\n",
    "\n",
    "# Assuming 'buss_corr' and 'eco_corr' are defined as your business and economy correlation matrices\n",
    "print('Business')\n",
    "df_business = adjust_top_5_values(df_business, buss_corr, delta_otp15_takeoff)\n",
    "print('Economy')\n",
    "df_economy = adjust_top_5_values(df_economy, eco_corr, delta_otp15_takeoff)\n",
    "\n",
    "# Optionally recombine the DataFrames and set new insert dates\n",
    "updated_df = pd.concat([df_business, df_economy]).sort_index()\n",
    "updated_df['insert_date_ci'] = '2024-05-08'  # Update insert date for all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c95ff-5ec7-46fd-aee5-1ed531315d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207ca41-c6d0-4c44-85bb-447e29fd5ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_df=pd.concat([year_targets_df,updated_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d8ed5-433b-4a01-94ef-7a1da49e38d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "day_predict_df = pd.read_csv('intervals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c3574-cf12-4d8b-b782-e6597e4dd90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd212dbb-2c03-4250-8de3-78cd6131c875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a4d1f-2675-4e2c-8b1b-7704fa217360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_df = pd.read_csv('targets.csv')\n",
    "\n",
    "targets_df=targets_df[targets_df['cabin']!='Global']\n",
    "\n",
    "targets_df['insert_date_ci']='2023-11-21'\n",
    "\n",
    "targets_df.rename(columns={\n",
    "    'cabin' : 'cabin_in_surveyed_flight',\n",
    "    'date_flight_local': 'end_date'  # Assuming you want to consider end_date as the equivalent of date_flight_local\n",
    "}, inplace=True)\n",
    "# day_predict_df.rename(columns={\n",
    "#     'interval_end_date': 'insert_date_ci'  # Assuming you want to consider end_date as the equivalent of date_flight_local\n",
    "# }, inplace=True)\n",
    "\n",
    "# Correct the conversion to datetime objects\n",
    "targets_df['end_date'] = pd.to_datetime(targets_df['end_date'])\n",
    "\n",
    "targets_df = targets_df[(targets_df['end_date'].dt.year == 2024)]\n",
    "\n",
    "# Compute 'start_date' as the first day of the corresponding month\n",
    "# Using dt.to_period('M').to_timestamp() to safely navigate datetime formats\n",
    "targets_df['start_date'] = targets_df['end_date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "targets_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ac9db-d961-45e8-9019-5ce9a40839f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monthly improvements\n",
    "month_improvements = {\n",
    "    1: 0.7, 2: 0.2, 3: 0.5, 4: 0.3, 5: 0.2, 6: 0.3,\n",
    "    7: 0.6, 8: 0.6, 9: 0.8, 10: 0.7, 11: 0.5, 12: 0.2\n",
    "}\n",
    "\n",
    "# Function to adjust otp15_takeoff and the top 5 correlated features excluding otp15_takeoff itself\n",
    "def adjust_otp_and_top_5_correlations(row, eco_corr, buss_corr):\n",
    "    # Select the appropriate correlation matrix\n",
    "    corr_matrix = buss_corr if row['cabin_in_surveyed_flight'] == 'Business' else eco_corr\n",
    "\n",
    "    # Identify the top 5 features based on absolute correlation with 'otp15_takeoff'\n",
    "    # Exclude 'otp15_takeoff' from being considered as one of the top correlated features\n",
    "    top_features = ['pfl_100_checkin_satisfaction', 'pfl_500_boarding_satisfaction', 'arr_100_arrivals_satisfaction', 'con_100_connections_satisfaction', 'ifl_100_cabin_crew_satisfaction']\n",
    "\n",
    "    # Apply monthly improvement\n",
    "    month = row['end_date'].month\n",
    "    delta_otp15_takeoff = month_improvements.get(month, 0)\n",
    "    row['otp15_takeoff'] += delta_otp15_takeoff\n",
    "    \n",
    "    # Apply correlation-based adjustments only to the top 5 correlated features\n",
    "    for column in top_features:\n",
    "        if column in row:\n",
    "            adjustment_factor = corr_matrix.at[column, 'otp15_takeoff'] * delta_otp15_takeoff\n",
    "            row[column] += adjustment_factor\n",
    "\n",
    "    row['insert_date_ci'] = '2024-05-08'\n",
    "    return row\n",
    "\n",
    "# Adjust the DataFrame\n",
    "adjusted_df = targets_df.apply(lambda row: adjust_otp_and_top_5_correlations(row.copy(), eco_corr, buss_corr), axis=1)\n",
    "concatenated_df = pd.concat([targets_df, adjusted_df]).sort_index(kind='merge')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a898c4-0e5a-4b41-9c1e-ef44098d2534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2ac70-f901-4050-b20f-e8554dea3d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_df =targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2747c-153c-40bb-9108-3e1fa16d6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df=all_weekly.copy()\n",
    "concatenated_df['insert_date_ci']='2024-06-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4cfec-6b88-4efd-87f6-6f47a5cb5874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba8320-665f-44a5-b3b9-033f3e7ee402",
   "metadata": {},
   "source": [
    "## Genetic algorithms simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f92c20-b27d-4818-bd3a-5704e1357ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cabin = 'Economy'\n",
    "haul = 'SH'\n",
    "\n",
    "model_file_path = os.path.join('targets_model', f\"best_tuned_mae_model_{cabin}_{haul}_df_LightGBMModel.pkl\")\n",
    "with open(model_file_path, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "    \n",
    "scaler_file_path = os.path.join('targets_model', f\"future_scaler_{cabin}_{haul}_df.pkl\")\n",
    "with open(scaler_file_path, 'rb') as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "    \n",
    "# Access the trained LightGBM model\n",
    "lgb_model = model.model\n",
    "feature_names = model.lagged_feature_names\n",
    "\n",
    "# Set feature names manually if not already set\n",
    "lgb_model.feature_name_ "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb6b7815-8082-4e8d-97c4-c6b3862d8ec0",
   "metadata": {},
   "source": [
    "#June 01/2024\n",
    "bounds = {'Economy_SH': {'desired_output': 40.8, 'ytd': np.array([\n",
    "    71.84856075258804, 79.06015584914005, 80.29968621466156, 74.394075935135,\n",
    "    76.02849620274152, 72.96724176680158, 78.49867970533091, 43.23740683347413,\n",
    "    44.49390640706396, 53.73307758411367, 81.1828997942445, 78.59234695868854,\n",
    "    69.37947451916642, 65.98255786642703, 53.1402991538944, 87.94953821163229,\n",
    "    88.62830908016969\n",
    "]) ,'lower_bounds': np.array([\n",
    "    71.84856075258804, 79.06015584914005, 80.29968621466156, 74.394075935135,\n",
    "    76.02849620274152, 72.96724176680158, 78.49867970533091, 43.23740683347413,\n",
    "    44.49390640706396, 53.73307758411367, 81.1828997942445, 78.59234695868854,\n",
    "    69.37947451916642, 65.98255786642703, 53.1402991538944, 87.94953821163229,\n",
    "    88.62830908016969\n",
    "]), 'upper_bounds': np.array([\n",
    "    81.84856075258804, 80.06015584914005, 90.29968621466156, 74.494075935135,\n",
    "    86.02849620274152, 82.96724176680158, 78.59867970533091, 43.33740683347413,\n",
    "    44.59390640706396, 53.83307758411367, 91.1828997942445, 88.59234695868854,\n",
    "    69.47947451916642, 66.08255786642703, 53.2402991538944, 88.04953821163229,\n",
    "    88.72830908016969\n",
    "])},\n",
    "'Business_SH': {'desired_output': 49.2, 'ytd': np.array([\n",
    "    75.23031137101951, 82.38024124297641, 81.05735897610812, 73.4348802421541,\n",
    "    78.16699370088098, 76.69004349622989, 80.57402608565235, 50.02440166461712,\n",
    "    43.7342726867898, 75.49615314832714, 88.3768021643377, 80.67670254574661,\n",
    "    73.27391902671584, 70.68348691307116, 54.63516058451109, 80.64093457772394,\n",
    "    88.91542568137118\n",
    "]) , 'lower_bounds': np.array([\n",
    "    75.23031137101951, 82.38024124297641, 81.05735897610812, 73.4348802421541,\n",
    "    78.16699370088098, 76.69004349622989, 80.57402608565235, 50.02440166461712,\n",
    "    43.7342726867898, 75.49615314832714, 88.3768021643377, 80.67670254574661,\n",
    "    73.27391902671584, 70.68348691307116, 54.63516058451109, 80.64093457772394,\n",
    "    88.91542568137118\n",
    "]), 'upper_bounds': np.array([\n",
    "    85.23031137101951, 92.38024124297641, 81.55735897610812, 73.9348802421541,\n",
    "    88.16699370088098, 86.69004349622989, 80.77402608565235, 50.22440166461712,\n",
    "    43.9342726867898, 85.49615314832714, 98.3768021643377, 90.67670254574661,\n",
    "    73.47391902671584, 70.88348691307116, 54.83516058451109, 80.84093457772394,\n",
    "    89.11542568137118\n",
    "])},\n",
    "'Economy_LH': {'desired_output': 35.2, 'ytd': np.array([\n",
    "    69.94692745768742, 76.37971508592283, 83.53404006597242, 75.45786142339006,\n",
    "    76.70701347444925, 69.23555942276643, 81.3690654233575, 45.73229361034287,\n",
    "    76.94215292944797, 61.3017951010311, 76.7189968110862, 79.28077952679668,\n",
    "    69.86890898191199, 65.51659033888608, 54.40695325711143, 89.16070046492685,\n",
    "    81.55314073681421\n",
    "]) , 'lower_bounds': np.array([\n",
    "    69.94692745768742, 76.37971508592283, 83.53404006597242, 75.45786142339006,\n",
    "    76.70701347444925, 69.23555942276643, 81.3690654233575, 45.73229361034287,\n",
    "    76.94215292944797, 61.3017951010311, 76.7189968110862, 79.28077952679668,\n",
    "    69.86890898191199, 65.51659033888608, 54.40695325711143, 89.16070046492685,\n",
    "    81.55314073681421\n",
    "]), 'upper_bounds': np.array([    \n",
    "    79.94692745768742, 86.37971508592283, 93.53404006597242, 75.65786142339006,\n",
    "    86.70701347444925, 79.23555942276643, 91.3690654233575, 45.93229361034287,\n",
    "    77.14215292944797, 71.3017951010311, 86.7189968110862, 89.28077952679668,\n",
    "    70.06890898191199, 65.71659033888608, 64.40695325711143, 89.36070046492685,\n",
    "    81.75314073681421\n",
    "])},\n",
    "'Business_LH': {'desired_output': 46.1, 'ytd': np.array([\n",
    "    74.72527185843502, 82.34036415093084, 83.61199247605892, 76.94601368088321,\n",
    "    78.09857519222611, 77.2145571380715, 82.48022875399207, 45.79028790966307,\n",
    "    72.0624335804457, 70.20649872673383, 82.4674726675342, 81.67688803804099,\n",
    "    72.35201853035389, 72.41368121055748, 60.700457388759055, 91.93078518699286,\n",
    "    81.84673366834171\n",
    "]) ,  'lower_bounds': np.array([\n",
    "    74.72527185843502, 82.34036415093084, 83.61199247605892, 76.94601368088321,\n",
    "    78.09857519222611, 77.2145571380715, 82.48022875399207, 45.79028790966307,\n",
    "    72.0624335804457, 70.20649872673383, 82.4674726675342, 81.67688803804099,\n",
    "    72.35201853035389, 72.41368121055748, 60.700457388759055, 91.93078518699286,\n",
    "    81.84673366834171\n",
    "\n",
    "]), 'upper_bounds': np.array([    \n",
    "    84.72527185843502, 92.34036415093084, 93.61199247605892, 86.94601368088321,\n",
    "    88.09857519222611, 87.2145571380715, 92.48022875399207, 45.99028790966307,\n",
    "    72.2624335804457, 80.20649872673383, 92.4674726675342, 91.67688803804099,\n",
    "    72.55201853035389, 82.51368121055748, 70.700457388759055, 93.43078518699286,\n",
    "    82.34673366834171\n",
    "])},\n",
    "'Premium Economy_LH': {'desired_output': 37.6, 'ytd': np.array([\n",
    "    70.6576888793063, 78.01593829687357, 83.50691270575301, 83.12038755022735,\n",
    "    78.3278425515155, 70.96599455839635, 80.13189788080383, 45.8697240615013,\n",
    "    74.96750406425723, 56.824687880788375, 74.26538429756921, 79.71272832155954,\n",
    "    71.19469065403004, 71.67231305578493, 52.30992733859235, 89.455383549599,\n",
    "    83.7037037037037\n",
    "]) , 'lower_bounds': np.array([\n",
    "    70.6576888793063, 78.01593829687357, 83.50691270575301, 83.12038755022735,\n",
    "    78.3278425515155, 70.96599455839635, 80.13189788080383, 45.8697240615013,\n",
    "    74.96750406425723, 56.824687880788375, 74.26538429756921, 79.71272832155954,\n",
    "    71.19469065403004, 71.67231305578493, 52.30992733859235, 89.455383549599,\n",
    "    83.7037037037037\n",
    "\n",
    "]), 'upper_bounds': np.array([    \n",
    "    80.6576888793063, 88.01593829687357, 93.50691270575301, 83.32038755022735,\n",
    "    88.3278425515155, 80.96599455839635, 90.13189788080383, 46.8697240615013,\n",
    "    79.96750406425723, 66.824687880788375, 84.26538429756921, 89.71272832155954,\n",
    "    71.39469065403004, 75.67231305578493, 57.30992733859235, 89.655383549599,\n",
    "    84.7037037037037\n",
    "])},\n",
    "          \n",
    "          \n",
    "}\n",
    "    # 79.94692745768742, 86.37971508592283, 93.53404006597242, 85.45786142339006,\n",
    "    # 86.70701347444925, 79.23555942276643, 91.3690654233575, 55.73229361034287,\n",
    "    # 86.94215292944797, 71.3017951010311, 86.7189968110862, 89.28077952679668,\n",
    "    # 79.86890898191199, 75.51659033888608, 64.40695325711143, 89.26070046492685,\n",
    "    # 91.55314073681421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433c6d5-8cd7-4fe6-9e97-ba3b1c608d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos que `corr_data` es tu DataFrame de datos\n",
    "corr_data = pd.read_csv('daily_NPS_2024-06-01.csv')\n",
    "variables = [\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\"\n",
    "]\n",
    "\n",
    "# Crear un diccionario para almacenar los límites\n",
    "bounds = {\n",
    "    'Economy_SH': {'desired_output': 40.8, 'ytd': np.array([\n",
    "        71.53872340864697, 78.75654554839483, 80.11452916215028, 74.04299984967855,\n",
    "        75.60965424745486, 72.8084702569169, 78.22524650876508, 43.00880046374199,\n",
    "        44.554300671622585, 53.439005754954906, 80.91934994215856, 78.1213030736794,\n",
    "        68.9443135427536, 65.82854958118871, 52.8143644885526, 87.98863696320771,\n",
    "        87.94872744785685\n",
    "    ])},\n",
    "    'Business_SH': {'desired_output': 49.2, 'ytd': np.array([\n",
    "        75.21281355971165, 82.01684831634871, 81.09703133030277, 72.74481985846293,\n",
    "        77.5257587435053, 76.5637379530902, 80.50212103977712, 50.18471338223737,\n",
    "        43.42476471205928, 75.21962008760555, 88.11284159064535, 80.01284943411335,\n",
    "        72.73886095505785, 70.24804759939938, 53.61973641048212, 82.88961363106925,\n",
    "        88.09584501656896\n",
    "    ])},\n",
    "    'Economy_LH': {'desired_output': 35.2, 'ytd': np.array([\n",
    "        69.45899014981862, 75.97682039298141, 83.21558066375458, 75.77040405189314,\n",
    "        76.13534099727835, 69.0042942723428, 81.203111247732, 45.392686630612594,\n",
    "        76.5714728414748, 61.18137866367399, 76.57786809158281, 78.80601620406625,\n",
    "        69.23713704858694, 65.081074046137, 53.6517910918804, 89.21558918425164,\n",
    "        80.51594150240712\n",
    "    ])},\n",
    "    'Business_LH': {'desired_output': 46.1, 'ytd': np.array([\n",
    "        73.74847921792322, 81.32798786888064, 83.08217829338287, 76.49730874868177,\n",
    "        77.26554399669993, 76.63356742362893, 82.55472749375869, 46.23855519118868,\n",
    "        71.9934182917934, 70.16725739263926, 82.41271972809379, 80.97557771556721,\n",
    "        70.99143320747852, 72.15848777646852, 59.78423355546649, 92.15834718930202,\n",
    "        80.58326937835764\n",
    "    ])},\n",
    "    'Premium Economy_LH': {'desired_output': 37.6, 'ytd': np.array([\n",
    "        70.47667460599726, 77.76567880595374, 83.49901673416883, 82.68836463802664,\n",
    "        77.8440324645451, 70.8471777826006, 80.15689019950972, 45.68048301381103,\n",
    "        74.83582622448148, 57.2564051306007, 74.43244367790237, 79.72244225578565,\n",
    "        70.86792808007696, 71.24113107741451, 51.64779408101883, 89.66540911621003,\n",
    "        83.0166270783848\n",
    "    ])},\n",
    "}\n",
    "\n",
    "# Definir los límites inferiores iniciales\n",
    "for cabin_haul in bounds.values():\n",
    "    cabin_haul['lower_bounds'] = cabin_haul['ytd'].copy()\n",
    "\n",
    "# Crear los límites superiores basados en las correlaciones con NPS_weighted\n",
    "for cabin_haul_key, cabin_haul in bounds.items():\n",
    "    # Filtrar datos por cabin y haul\n",
    "    filtered_data = corr_data[(corr_data['cabin'] == cabin_haul_key.split('_')[0]) & \n",
    "                              (corr_data['haul'] == cabin_haul_key.split('_')[1])]\n",
    "    if filtered_data.empty:\n",
    "        print(f\"No data for {cabin_haul_key}\")\n",
    "        continue\n",
    "    \n",
    "    # Calcular la matriz de correlación\n",
    "    corr_matrix = filtered_data.corr()\n",
    "\n",
    "    # Inicializar upper_bounds con los valores ytd\n",
    "    upper_bounds = cabin_haul['ytd'].copy()\n",
    "\n",
    "    # Aplicar las modificaciones basadas en las correlaciones con NPS_weighted\n",
    "    for i, var in enumerate(variables):\n",
    "        correlation = corr_matrix.loc[var, 'NPS_weighted']\n",
    "        if var in ['load_factor', 'otp15_takeoff']:\n",
    "            upper_bounds[i] += 0.2\n",
    "        elif correlation > 0.5:\n",
    "            upper_bounds[i] += 3\n",
    "        elif correlation > 0.4:\n",
    "            upper_bounds[i] += 1\n",
    "        else:\n",
    "            upper_bounds[i] += 0.2\n",
    "\n",
    "    cabin_haul['upper_bounds'] = upper_bounds\n",
    "\n",
    "bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275cd67-14ea-46bd-82f3-4bbe883ff7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cabin='Premium Economy'\n",
    "haul = 'LH'\n",
    "corr_data = pd.read_csv('daily_aggregation.csv')\n",
    "corr_data = corr_data[corr_data['start_date']>='2023-01-01']\n",
    "variables = [\n",
    "    'pun_100_punctuality_satisfaction',\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\"\n",
    "]\n",
    "# corr_matrix = corr_data[(corr_data['cabin']==cabin) & (corr_data['haul']==haul)][variables+['NPS_weighted']].corr()\n",
    "corr_matrix = corr_data[(corr_data['cabin_in_surveyed_flight']==cabin) & (corr_data['haul']==haul)][['pun_100_punctuality_satisfaction', 'otp15_takeoff', 'NPS_weighted']].corr()\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ced84-01d9-4cfc-938c-754ef3670a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_data['start_date'] = pd.to_datetime(corr_data['start_date'])\n",
    "\n",
    "filtered_df = corr_data[(corr_data['cabin_in_surveyed_flight'] == cabin) & (corr_data['haul'] == haul)]\n",
    "\n",
    "# Crear una columna de semana del año\n",
    "filtered_df['week'] = filtered_df['start_date'].dt.isocalendar().week\n",
    "filtered_df['year'] = filtered_df['start_date'].dt.year\n",
    "\n",
    "# Agrupar por año y semana, y tomar la media de los valores\n",
    "weekly_df = filtered_df.groupby(['year', 'week']).agg({\n",
    "    'otp15_takeoff': 'mean',\n",
    "    'pun_100_punctuality_satisfaction': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Crear una columna de fecha representativa para la semana\n",
    "weekly_df['date'] = pd.to_datetime(weekly_df['year'].astype(str) + '-W' + weekly_df['week'].astype(str) + '-1', format='%Y-W%W-%w')\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot 'otp15_takeoff'\n",
    "plt.plot(weekly_df['date'], weekly_df['otp15_takeoff'], label='OTP 15 Takeoff', color='blue', marker='o')\n",
    "\n",
    "# Plot 'pun_100_punctuality'\n",
    "plt.plot(weekly_df['date'], weekly_df['pun_100_punctuality_satisfaction'], label='Punctuality satisfaction', color='green', marker='o')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.title('OTP 15 Takeoff and Punctuality satisfaction over Time (Weekly)')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618abbc5-445a-4963-9c68-179425c4c56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def scatter_plot_with_regression(df, variable, target):\n",
    "    # Remove rows with missing values\n",
    "    df_clean = df.dropna(subset=[variable, target])\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(10, 10))  # Square plot\n",
    "    plt.scatter(df_clean[variable], df_clean[target], alpha=0.6, edgecolors='w', linewidths=0.5, label='Data points')\n",
    "    \n",
    "    # Fit the linear regression model\n",
    "    X = df_clean[variable].values.reshape(-1, 1)\n",
    "    y = df_clean[target].values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Plot the regression line\n",
    "    plt.plot(df_clean[variable], model.predict(X), color='red', label='Fitted line')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(f'{variable} vs {target}')\n",
    "    plt.xlabel(f'{variable}')\n",
    "    plt.ylabel(f'{target}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Calculate R^2\n",
    "    r_squared = model.score(X, y)\n",
    "    \n",
    "    # Add slope and R^2 to the plot\n",
    "    plt.text(0.05, 0.10, f'Slope: {model.coef_[0]:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.text(0.05, 0.05, f'R^2: {r_squared:.4f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return model.coef_[0], model.intercept_, r_squared\n",
    "scatter_df = corr_data[(corr_data['cabin_in_surveyed_flight']==cabin) & (corr_data['haul']==haul)]\n",
    "scatter_plot_with_regression(scatter_df, 'otp15_takeoff', 'NPS_weighted' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40168918-219d-49c6-8b82-25e454aacef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume regressor and darts_scaler are already defined and trained\n",
    "regressor = lgb_model\n",
    "darts_scaler = scaler\n",
    "\n",
    "# Define desired output and bounds\n",
    "desired_output = bounds[f'{cabin}_{haul}']['desired_output']\n",
    "threshold = 0.05\n",
    "lower_bounds = bounds[f'{cabin}_{haul}']['lower_bounds']\n",
    "upper_bounds = bounds[f'{cabin}_{haul}']['upper_bounds']\n",
    "\n",
    "# Load correlation data\n",
    "corr_data = pd.read_csv('daily_NPS_2024-06-01.csv')\n",
    "corr_data = corr_data[(corr_data['cabin'] == cabin) & (corr_data['haul'] == haul)][[\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\"\n",
    "]]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = corr_data.corr()\n",
    "\n",
    "# Define fitness function\n",
    "def fitness_function(individual):\n",
    "    series = TimeSeries.from_values(np.array(individual).reshape(1, -1))\n",
    "    x_scaled = darts_scaler.transform(series)\n",
    "    prediction = regressor.predict(x_scaled.values())[0]\n",
    "    \n",
    "    # Calculate the cost (assuming it's the deviation from a baseline or target)\n",
    "    actual_values = bounds[f'{cabin}_{haul}']['ytd']\n",
    "    cost = mean_squared_error(np.array(individual), actual_values)\n",
    "    \n",
    "    # Prioritize prediction error, then cost\n",
    "    prediction_error = abs(prediction - desired_output)\n",
    "    \n",
    "    # Combine the two objectives into one fitness value\n",
    "    return prediction_error, cost\n",
    "\n",
    "# Check bounds function\n",
    "def check_bounds(individual, lower_bounds, upper_bounds):\n",
    "    for i in range(len(individual)):\n",
    "        if individual[i] < lower_bounds[i]:\n",
    "            individual[i] = lower_bounds[i]\n",
    "        elif individual[i] > upper_bounds[i]:\n",
    "            individual[i] = upper_bounds[i]\n",
    "    return individual\n",
    "\n",
    "# Check correlations function\n",
    "def check_correlations(individual, correlation_matrix, tolerance=0.1):\n",
    "    num_vars = len(individual)\n",
    "    for i in range(num_vars):\n",
    "        for j in range(i + 1, num_vars):\n",
    "            expected_corr = correlation_matrix.iloc[i, j]\n",
    "            actual_corr = np.corrcoef(individual[i], individual[j])[0, 1]\n",
    "            if abs(actual_corr - expected_corr) > tolerance:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# DEAP configuration for multi-objective optimization\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0, -1.0))  # Minimizing both objectives\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "def create_individual():\n",
    "    return [np.random.uniform(low, high) for low, high in zip(lower_bounds, upper_bounds)]\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "\n",
    "# Custom mutation function\n",
    "def custom_mutate(individual, eta=1.0, low=lower_bounds, up=upper_bounds, indpb=0.2):\n",
    "    size = len(individual)\n",
    "    for i in range(size):\n",
    "        if np.random.random() <= indpb:\n",
    "            x = individual[i]\n",
    "            xl = low[i]\n",
    "            xu = up[i]\n",
    "            delta_1 = (x - xl) / (xu - xl)\n",
    "            delta_2 = (xu - x) / (xu - xl)\n",
    "            mut_pow = 1.0 / (eta + 1.0)\n",
    "            rand = np.random.random()\n",
    "            if rand < 0.5:\n",
    "                xy = 1.0 - delta_1\n",
    "                val = 2.0 * rand + (1.0 - 2.0 * rand) * (xy ** (eta + 1))\n",
    "                delta_q = val ** mut_pow - 1.0\n",
    "            else:\n",
    "                xy = 1.0 - delta_2\n",
    "                val = 2.0 * (1.0 - rand) + 2.0 * (rand - 0.5) * (xy ** (eta + 1))\n",
    "                delta_q = 1.0 - val ** mut_pow\n",
    "            x = x + delta_q * (xu - xl)\n",
    "            x = min(max(x, xl), xu)\n",
    "            individual[i] = x\n",
    "    return individual,\n",
    "\n",
    "# Custom crossover function with bounds check\n",
    "def custom_cxBlend(ind1, ind2, alpha=0.5):\n",
    "    ind1, ind2 = tools.cxBlend(ind1, ind2, alpha)\n",
    "    ind1 = check_bounds(ind1, lower_bounds, upper_bounds)\n",
    "    ind2 = check_bounds(ind2, lower_bounds, upper_bounds)\n",
    "    return ind1, ind2\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutate)\n",
    "toolbox.register(\"mate\", custom_cxBlend, alpha=0.5)\n",
    "toolbox.register(\"select\", tools.selNSGA2)  # Using NSGA-II for multi-objective optimization\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    population = toolbox.population(n=500)\n",
    "    ngen = 50\n",
    "    cxpb = 0.5\n",
    "    mutpb = 0.2\n",
    "\n",
    "    valid_individuals = []\n",
    "    best_individual = None\n",
    "    best_fitness = (float('inf'), float('inf'))\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Verificar y corregir límites después de las operaciones genéticas\n",
    "        for ind in offspring:\n",
    "            check_bounds(ind, lower_bounds, upper_bounds)\n",
    "\n",
    "        fits = map(toolbox.evaluate, offspring)\n",
    "\n",
    "        for fit, ind in zip(fits, offspring):\n",
    "            ind.fitness.values = fit\n",
    "            if fit[0] < threshold:\n",
    "                ind = check_bounds(ind, lower_bounds, upper_bounds)  # Verificación final\n",
    "                # and check_correlations(ind, correlation_matrix)\n",
    "                if all(lower_bounds[i] <= ind[i] <= upper_bounds[i] for i in range(len(ind))):\n",
    "                    valid_individuals.append((ind, fit[0], fit[1]))  # Append cost as well\n",
    "\n",
    "            # Track the best individual\n",
    "            if fit < best_fitness:\n",
    "                best_individual = ind\n",
    "                best_fitness = fit\n",
    "\n",
    "        population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "    return population, valid_individuals, best_individual\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_population, valid_individuals, best_individual = main()\n",
    "    \n",
    "    if valid_individuals:\n",
    "        print(f\"Found {len(valid_individuals)} valid individuals within the threshold.\")\n",
    "        for ind, pred_error, cost in valid_individuals:\n",
    "            print(f\"Individual: {ind}, Prediction Error: {pred_error}, Cost: {cost}\")\n",
    "    else:\n",
    "        print(\"No valid individuals found within the threshold.\")\n",
    "    \n",
    "    if best_individual:\n",
    "        best_individual_fitness = fitness_function(best_individual)\n",
    "        print(f\"Best individual: {best_individual}, Prediction Error: {best_individual_fitness[0]}, Cost: {best_individual_fitness[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93be8ed-1061-4ea0-9780-10a613a39a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supongamos que estos son los nombres de las características de tus datos\n",
    "feature_names =  [\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"cabin_in_surveyed_flight\",\n",
    "    \"haul\",\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\",\n",
    "    \"NPS_weighted\",\n",
    "    \"insert_date_ci\"\n",
    "]\n",
    "\n",
    "# Datos del YTD (Year-To-Date)\n",
    "# ytd_2024 = [\n",
    "#     \"2024-01-01\",\n",
    "#     \"2024-06-01\",\n",
    "#     \"Economy\",\n",
    "#     \"SH\",\n",
    "#     71.84856075258804,\n",
    "#     79.06015584914005,\n",
    "#     80.29968621466156,\n",
    "#     74.394075935135,\n",
    "#     76.02849620274152,\n",
    "#     72.96724176680158,\n",
    "#     78.49867970533091,\n",
    "#     43.23740683347413,\n",
    "#     44.49390640706396,\n",
    "#     53.73307758411367,\n",
    "#     81.1828997942445,\n",
    "#     78.59234695868854,\n",
    "#     69.37947451916642,\n",
    "#     65.98255786642703,\n",
    "#     53.1402991538944,\n",
    "#     87.94953821163229,\n",
    "#     88.62830908016969,\n",
    "#     40.4,\n",
    "#     '2024-06-01'\n",
    "# ]\n",
    "# Datos del YTD (Year-To-Date)\n",
    "# ytd_2024 = [\n",
    "#     \"2024-01-01\",\n",
    "#     \"2024-06-01\",\n",
    "#     \"Business\",\n",
    "#     \"SH\",\n",
    "#     75.23031137101951, 82.38024124297641, 81.05735897610812, 73.4348802421541,\n",
    "#     78.16699370088098, 76.69004349622989, 80.57402608565235, 50.02440166461712,\n",
    "#     43.7342726867898, 75.49615314832714, 88.3768021643377, 80.67670254574661,\n",
    "#     73.27391902671584, 70.68348691307116, 54.63516058451109, 82.64093457772394,\n",
    "#     88.91542568137118,\n",
    "#     47.3,\n",
    "#     '2024-06-01'\n",
    "# ]\n",
    "print(lower_bounds)\n",
    "ytd_2024 = ['2024-01-01', '2024-06-01', cabin, haul] + lower_bounds.tolist() + [bounds[f'{cabin}_{haul}']['desired_output'], '2024-06-01']\n",
    "\n",
    "# Convertir YTD en un DataFrame\n",
    "ytd_data = {feature: value for feature, value in zip(feature_names, ytd_2024)}\n",
    "ytd_df = pd.DataFrame([ytd_data])\n",
    "\n",
    "# Supongamos que `valid_individuals` es el resultado del algoritmo genético\n",
    "# valid_individuals = [(ind, fit) for ind, fit in ... ]  # Aquí tienes tus individuos válidos y sus fitness\n",
    "\n",
    "# Convertir individuos válidos en DataFrames y concatenar\n",
    "valid_dfs = []\n",
    "for ind, fit, cost in valid_individuals:\n",
    "    individual = ind + [f'{desired_output}', '2024-06-19']\n",
    "    best_individual = best_individual + ['2024-06-19']\n",
    "    values = ['2024-01-01', '2024-06-01', cabin, haul] + best_individual\n",
    "    data = {feature: value for feature, value in zip(feature_names, values)}\n",
    "    df = pd.DataFrame([data])\n",
    "    valid_dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames válidos con el YTD\n",
    "concatenated_df = pd.concat([ytd_df] + valid_dfs).sort_index(kind='merge')\n",
    "\n",
    "# Mostrar el DataFrame concatenado\n",
    "concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a480829-7ecc-4190-906e-f41627c379d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    # df.drop(columns=['pun_100_punctuality_satisfaction', 'inm_400_issues_response_satisfaction'], inplace=True)\n",
    "    # Agrupar y procesar los datos\n",
    "    grouped_dfs = {}\n",
    "    features = {}\n",
    "    for group_name, group_data in df.groupby(['cabin_in_surveyed_flight', 'haul']):\n",
    "        cabin_value, haul_value = group_name\n",
    "        group_df = group_data.copy()\n",
    "        group_df_name = f'{cabin_value}_{haul_value}_df'\n",
    "        \n",
    "        # Identificar las columnas de características\n",
    "        satisfaction_cols = [col for col in df.columns if col.endswith('_satisfaction')]\n",
    "        otp_cols = ['otp15_takeoff']\n",
    "        features_cols = satisfaction_cols + ['load_factor'] + otp_cols\n",
    "        cols_to_keep = ['insert_date_ci', 'start_date','end_date','cabin_in_surveyed_flight', 'haul'] + features_cols + ['NPS_weighted']\n",
    "\n",
    "        # Filtrar las columnas en el grupo y actualizar el diccionario de características\n",
    "        grouped_df = group_df[cols_to_keep]\n",
    "        features[group_df_name] = features_cols\n",
    "        grouped_dfs[group_df_name] = grouped_df\n",
    "\n",
    "    # Reconstruir el DataFrame original\n",
    "    df = pd.concat(grouped_dfs.values())\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df, grouped_dfs, features\n",
    "\n",
    "# Aplicar la función a cada DataFrame y almacenar los resultados en las variables correspondientes\n",
    "day_predict_df, day_predict_df_grouped_dfs, features_cols = process_dataframe(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39f358-d22f-425f-b536-f29fff7cdb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "day_predict_df_grouped_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5275ee0-54a2-41db-9706-45f39efd904f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfbc77-7a9b-474a-9e3d-3f6fa1f7b83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "satisfaction_cols = [col for col in day_predict_df.columns if col.endswith('_satisfaction')]\n",
    "otp_cols = ['otp15_takeoff']\n",
    "features_cols = satisfaction_cols + ['load_factor'] + otp_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789b7d2-ef44-4aa9-9fcd-31084de6bc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93ad5a-ae42-4e3a-bc57-65cd2cc0566a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from darts.timeseries import TimeSeries\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def compute_shap_and_prediction(row, key, features_cols):\n",
    "    \"\"\"\n",
    "    Computes SHAP values and the predicted NPS for a given row.\n",
    "    \n",
    "    Parameters:\n",
    "    - row_df: The DataFrame row for which to compute SHAP values and prediction.\n",
    "    - key: The key identifying the specific model and scaler to use.\n",
    "    - features_cols: List of column names representing features used by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing SHAP values as a dictionary and the predicted NPS.\n",
    "    \"\"\"\n",
    "    # Logic to prepare the row for SHAP value computation and prediction\n",
    "    aux_nps_ts = TimeSeries.from_series(pd.Series([0]))\n",
    "    aux_row = pd.DataFrame(0, index=[0], columns=row.columns)\n",
    "    row_df = pd.concat([aux_row, row]).reset_index(drop=True)\n",
    "    \n",
    "    # Load the pre-trained model and scaler\n",
    "    best_tuned_model_dataframe_path = os.path.join('targets_model', f\"best_tuned_dataframe_{key}.pkl\")\n",
    "    with open(best_tuned_model_dataframe_path, 'rb') as dataframe_file:\n",
    "        best_tuned_model = pickle.load(dataframe_file)\n",
    "    \n",
    "    future_scaler_path = os.path.join('targets_model', f\"future_scaler_{key}.pkl\")\n",
    "    with open(future_scaler_path, 'rb') as scaler_file:\n",
    "        future_scaler = pickle.load(scaler_file)\n",
    "    \n",
    "    future_covariates_ts = TimeSeries.from_dataframe(row_df[features_cols])[-1:]\n",
    "    future_covariates_ts_scaled = future_scaler.transform(future_covariates_ts)\n",
    "    \n",
    "    model_file_path = os.path.join('targets_model', f\"best_tuned_mae_model_{key}_{best_tuned_model['model_name']}.pkl\")\n",
    "    with open(model_file_path, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "    \n",
    "    # Compute SHAP values and prediction\n",
    "    shap_explain = ShapExplainer(model=model)\n",
    "    shap_explained = shap_explain.explain(aux_nps_ts, foreground_future_covariates=future_covariates_ts_scaled)\n",
    "    shap_explanation = shap_explained.get_shap_explanation_object(horizon=1)\n",
    "\n",
    "    shap_values = shap_explanation[0].values\n",
    "    base_value = shap_explanation[0].base_values\n",
    "    pred_value = base_value + shap_values.sum()\n",
    "    feature_names=[]\n",
    "    for feat in shap_explanation.feature_names:\n",
    "        name = [f for f in features_cols if f in feat]\n",
    "        feature_names.append(name[0])\n",
    "    \n",
    "    \n",
    "    # Convert SHAP values to a dictionary and adjust the logic based on your ShapExplainer\n",
    "    shap_values_dict = {f\"{feature}_nps\": value for feature, value in zip(feature_names, shap_values)}\n",
    "    shap_values_dict[\"out_prob_base\"] = base_value,\n",
    "    shap_values_dict[\"out_prob_nps\"] = pred_value,\n",
    "    \n",
    "    # print(row_df.loc[1,features_cols])\n",
    "    \n",
    "    shap_explanation = shap.Explanation(values=shap_values, \n",
    "                                 base_values=base_value, \n",
    "                                 data=np.array(row_df.loc[1,features_cols].values.flatten().tolist()), \n",
    "                                 feature_names=shap_explanation.feature_names)\n",
    "    \n",
    "    return shap_values_dict, shap_explanation, shap_explain, shap_explained\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the augmented DataFrames\n",
    "augmented_dfs = {}\n",
    "explanations = {}\n",
    "\n",
    "for key in day_predict_df_grouped_dfs.keys():\n",
    "    # Initialize a list to collect augmented rows\n",
    "    augmented_rows = []\n",
    "    explanations[key]={}\n",
    "    \n",
    "    n = len(day_predict_df_grouped_dfs[key])\n",
    "    # n=2\n",
    "    for index in range(n):\n",
    "        # Access the row by its index using .iloc\n",
    "        row_df = day_predict_df_grouped_dfs[key].iloc[[index]]\n",
    "\n",
    "        # Compute SHAP values and predicted NPS here...\n",
    "        # Assuming `compute_shap_and_prediction` is a function you'd implement\n",
    "        # This function should return SHAP values as a dict and the predicted NPS\n",
    "        shap_values, explanations[key][index], shap_explain, shap_explained = compute_shap_and_prediction(row_df, key, features_cols)\n",
    "        # shap.plots.waterfall(explanations[key][index], max_display=20)\n",
    "        # Generate summary plot\n",
    "        # shap.summary_plot(model)\n",
    "        # Display the plot\n",
    "        # plt.show()\n",
    "        \n",
    "        # For each feature, add its SHAP value to the row\n",
    "        for feature_name, shap_value in shap_values.items():\n",
    "            row_df[f'{feature_name}'] = shap_value\n",
    "\n",
    "        # Add base value and predicted NPS columns\n",
    "        # row_df['Base Value'] = shap_values['base_value']  # Adjust based on how you obtain the base value\n",
    "        # row_df['Predicted NPS'] = predicted_nps\n",
    "        # print(key)\n",
    "        # shap_explain.summary_plot()\n",
    "\n",
    "        # Append the augmented row to the list\n",
    "        augmented_rows.append(row_df)\n",
    "        \n",
    "\n",
    "    # Concatenate all augmented rows to form the complete augmented DataFrame\n",
    "    augmented_dfs[key] = pd.concat(augmented_rows).reset_index(drop=True)\n",
    "\n",
    "# `augmented_dfs` now contains the augmented DataFrames with SHAP values and predictions\n",
    "augmented_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b179a-d101-4182-8a1e-51af4b9067fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_uplifting_explanation(explanation2, explanation1):\n",
    "    \"\"\"\n",
    "    Create a new Explanation object representing the uplifting between two Explanation objects.\n",
    "\n",
    "    Parameters:\n",
    "        - explanation1: The first shap.Explanation object.\n",
    "        - explanation2: The second shap.Explanation object.\n",
    "\n",
    "    Returns:\n",
    "        - A new shap.Explanation object representing the uplifting.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in values, base_values, and data\n",
    "    diff_values = explanation2.values - explanation1.values\n",
    "    \n",
    "    diff_base_values = explanation1.base_values + sum(explanation1.values)\n",
    "    diff_data = explanation2.data - explanation1.data\n",
    "\n",
    "    # Create a new Explanation object with the difference values\n",
    "    diff_explanation = shap.Explanation(values=diff_values, base_values=diff_base_values, data=diff_data,\n",
    "                                        feature_names=explanation1.feature_names)\n",
    "\n",
    "    return diff_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b6668-8856-4c36-a975-eb602b67b806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(1,n):\n",
    "    diff_explanation = create_uplifting_explanation(explanations[f'{cabin}_{haul}_df'][i], explanations[f'{cabin}_{haul}_df'][0])\n",
    "    shap.plots.waterfall(diff_explanation, max_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294797a-c59f-444f-bc92-d866301137e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd6db-2137-40a3-a411-832c162695d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b2e3c-d78e-468e-a5de-dda8f4f71328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ee224-7e5a-4cdf-8048-a93ea2583306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e1ec7-6ae3-45b9-9273-0599e5f71e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Definición del NPS Global objetivo\n",
    "nps_global_objetivo = 40\n",
    "\n",
    "# Porcentajes de cada categoría\n",
    "porcentajes = np.array([0.23, 0.688, 0.046, 0.023, 0.013])\n",
    "\n",
    "# NPS iniciales (por ejemplo, podrían ser los valores actuales)\n",
    "nps_inicial = np.array([25.2, 24.8, 36.7, 40.5, 38.3])\n",
    "\n",
    "# Función objetivo para minimizar\n",
    "def objetivo(nps):\n",
    "    return np.abs(nps_global_objetivo - np.sum(porcentajes * nps))\n",
    "\n",
    "# Restricciones\n",
    "constraints = [{'type': 'eq', 'fun': lambda nps: np.sum(porcentajes * nps) - nps_global_objetivo}]\n",
    "\n",
    "# Optimización\n",
    "resultado = minimize(objetivo, nps_inicial, constraints=constraints)\n",
    "\n",
    "nps_optimizado = resultado.x\n",
    "nps_optimizado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2f71a-6393-4fc4-aa71-6507eba14207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nps_ch = [31.2967217 , 43.03715398, 37.91934434, 41.10967217, 38.64459759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b749eb9-9831-4ed4-8387-dc8a4b5ab5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "porcentajes * nps_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fa78f-8231-4019-8248-c11b615176df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Supongamos que `regressor` es tu modelo LightGBM entrenado y `darts_scaler` es tu escalador\n",
    "regressor = lgb_model\n",
    "darts_scaler = scaler  # Asegúrate de que este escalador esté ajustado\n",
    "\n",
    "# Definir el objetivo deseado\n",
    "desired_output = 40.8  # Valor objetivo\n",
    "threshold = 0.5  # Umbral para la condición de individuos con una buena predicción\n",
    "\n",
    "# Ejemplo de límites inferiores y superiores específicos para cada variable\n",
    "lower_bounds = np.array([\n",
    "    71.84856075258804, 79.06015584914005, 80.29968621466156, 74.394075935135,\n",
    "    76.02849620274152, 72.96724176680158, 78.49867970533091, 43.23740683347413,\n",
    "    44.49390640706396, 53.73307758411367, 81.1828997942445, 78.59234695868854,\n",
    "    69.37947451916642, 65.98255786642703, 53.1402991538944, 87.94953821163229,\n",
    "    88.62830908016969\n",
    "])\n",
    "upper_bound = 100\n",
    "\n",
    "# Definir la función de fitness\n",
    "def fitness_function(individual):\n",
    "    series = TimeSeries.from_values(np.array(individual).reshape(1, -1))\n",
    "    x_scaled = darts_scaler.transform(series)\n",
    "    prediction = regressor.predict(x_scaled.values())[0]\n",
    "    return abs(prediction - desired_output),\n",
    "\n",
    "# Configurar DEAP\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Queremos minimizar el error\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "def create_individual():\n",
    "    return [np.random.uniform(low, upper_bound) for low in lower_bounds]\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "\n",
    "# Función de mutación personalizada\n",
    "def custom_mutate(individual, eta=1.0, low=lower_bounds, up=upper_bound, indpb=0.2):\n",
    "    size = len(individual)\n",
    "    for i in range(size):\n",
    "        if np.random.random() <= indpb:\n",
    "            x = individual[i]\n",
    "            xl = low[i]\n",
    "            xu = up\n",
    "            delta_1 = (x - xl) / (xu - xl)\n",
    "            delta_2 = (xu - x) / (xu - xl)\n",
    "            mut_pow = 1.0 / (eta + 1.0)\n",
    "            rand = np.random.random()\n",
    "            if rand < 0.5:\n",
    "                xy = 1.0 - delta_1\n",
    "                val = 2.0 * rand + (1.0 - 2.0 * rand) * (xy ** (eta + 1))\n",
    "                delta_q = val ** mut_pow - 1.0\n",
    "            else:\n",
    "                xy = 1.0 - delta_2\n",
    "                val = 2.0 * (1.0 - rand) + 2.0 * (rand - 0.5) * (xy ** (eta + 1))\n",
    "                delta_q = 1.0 - val ** mut_pow\n",
    "            x = x + delta_q * (xu - xl)\n",
    "            x = min(max(x, xl), xu)\n",
    "            individual[i] = x\n",
    "    return individual,\n",
    "\n",
    "# Función para verificar y corregir límites\n",
    "def check_bounds(individual, lower_bounds, upper_bound):\n",
    "    for i in range(len(individual)):\n",
    "        if individual[i] < lower_bounds[i]:\n",
    "            individual[i] = lower_bounds[i]\n",
    "        elif individual[i] > upper_bound:\n",
    "            individual[i] = upper_bound\n",
    "    return individual\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Algoritmo Genético\n",
    "def main():\n",
    "    population = toolbox.population(n=300)\n",
    "    ngen = 40\n",
    "    cxpb = 0.5\n",
    "    mutpb = 0.2\n",
    "\n",
    "    valid_individuals = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Verificar y corregir límites después de las operaciones genéticas\n",
    "        for ind in offspring:\n",
    "            check_bounds(ind, lower_bounds, upper_bound)\n",
    "\n",
    "        fits = map(toolbox.evaluate, offspring)\n",
    "\n",
    "        for fit, ind in zip(fits, offspring):\n",
    "            ind.fitness.values = fit\n",
    "            if fit[0] < threshold:\n",
    "                if all(lower_bounds[i] <= ind[i] <= upper_bound for i in range(len(ind))):\n",
    "                    valid_individuals.append((ind, fit[0]))\n",
    "\n",
    "        population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "    return population, valid_individuals\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_population, valid_individuals = main()\n",
    "    \n",
    "    if valid_individuals:\n",
    "        print(f\"Found {len(valid_individuals)} valid individuals within the threshold.\")\n",
    "        for ind, fit in valid_individuals:\n",
    "            print(f\"Individual: {ind}, Fitness: {fit}\")\n",
    "    else:\n",
    "        print(\"No valid individuals found within the threshold.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a59e3-6094-48c1-9a17-73bcc7fc95e5",
   "metadata": {},
   "source": [
    "### With correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256cadd-f1e3-488c-b708-2c7480be25f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d09bed-f1f8-48bc-9a22-51dbc30f24ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e87784-a0e9-46f0-aa92-891c335fe5ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eb70c-26d1-455d-969c-b1f78a717cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "abe40c0c-a1cd-4ffc-9c87-abf0732d98a0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Supongamos que `regressor` es tu modelo LightGBM entrenado y `darts_scaler` es tu escalador\n",
    "regressor = lgb_model\n",
    "darts_scaler = scaler  # Asegúrate de que este escalador esté ajustado\n",
    "\n",
    "# Definir el objetivo deseado\n",
    "desired_output = 40.8  # Valor objetivo\n",
    "threshold = 0.05  # Umbral para la condición\n",
    "\n",
    "# Definir la función de fitness\n",
    "def fitness_function(individual):\n",
    "    series = TimeSeries.from_values(np.array(individual).reshape(1, -1))\n",
    "    x_scaled = darts_scaler.transform(series)\n",
    "    prediction = regressor.predict(x_scaled.values())[0]\n",
    "    return abs(prediction - desired_output),\n",
    "\n",
    "# Configuración del algoritmo genético\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", np.random.uniform, 0, 100)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=regressor.n_features_)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=50, sigma=25, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Función para aplicar límites específicos a cada variable de los individuos\n",
    "def apply_bounds(individual, lower_bounds, upper_bound = 100):\n",
    "    for i in range(len(individual)):\n",
    "        if individual[i] < lower_bounds[i]:\n",
    "            individual[i] = lower_bounds[i]\n",
    "        elif individual[i] > upper_bound:\n",
    "            individual[i] = upper_bound\n",
    "    return individual\n",
    "\n",
    "# Ejemplo de límites inferiores y superiores específicos para cada variable\n",
    "lower_bounds = [\n",
    "    71.84856075258804, 79.06015584914005, 80.29968621466156, 74.394075935135,\n",
    "    76.02849620274152, 72.96724176680158, 78.49867970533091, 43.23740683347413,\n",
    "    44.49390640706396, 53.73307758411367, 81.1828997942445, 78.59234695868854,\n",
    "    69.37947451916642, 65.98255786642703, 53.1402991538944, 87.94953821163229,\n",
    "    88.62830908016969\n",
    "]\n",
    "upper_bound = 100\n",
    "\n",
    "# Parámetros del algoritmo genético\n",
    "population = toolbox.population(n=10000)\n",
    "ngen, cxpb, mutpb = 1000, 0.5, 0.2  # Número de generaciones, probabilidad de cruce, probabilidad de mutación\n",
    "\n",
    "# Lista para guardar los individuos que cumplen la condición del umbral\n",
    "satisfying_individuals = []\n",
    "\n",
    "# Bucle evolutivo\n",
    "for gen in range(ngen):\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if np.random.rand() < cxpb:\n",
    "            toolbox.mate(child1, child2)\n",
    "            apply_bounds(child1, lower_bounds, upper_bound)\n",
    "            apply_bounds(child2, lower_bounds, upper_bound)\n",
    "        if np.random.rand() < mutpb:\n",
    "            toolbox.mutate(child1)\n",
    "            toolbox.mutate(child2)\n",
    "            apply_bounds(child1, lower_bounds, upper_bound)\n",
    "            apply_bounds(child2, lower_bounds, upper_bound)\n",
    "\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "        # Verificar si el individuo cumple con la condición del umbral\n",
    "        if fit[0] < threshold:\n",
    "            satisfying_individuals.append(ind)\n",
    "\n",
    "    population[:] = offspring\n",
    "\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "\n",
    "print(\"Mejor individuo:\", best_individual)\n",
    "print(\"Predicción del mejor individuo:\", fitness_function(best_individual))\n",
    "\n",
    "print(f\"Individuos que cumplen con el umbral de {threshold}:\")\n",
    "for ind in satisfying_individuals:\n",
    "    print(ind, fitness_function(ind))\n",
    "\n",
    "# Comparación detallada\n",
    "for ind in satisfying_individuals:\n",
    "    prediction = fitness_function(ind)\n",
    "    print(f\"Individuo: {ind}, Predicción: {prediction}, Fitness: {prediction[0]}\")\n",
    "\n",
    "# Revalidar la predicción del mejor individuo\n",
    "series_best = TimeSeries.from_values(np.array(best_individual).reshape(1, -1))\n",
    "x_scaled_best = darts_scaler.transform(series_best)\n",
    "prediction_best = regressor.predict(x_scaled_best.values())[0]\n",
    "print(f\"Revalidación - Mejor individuo: {best_individual}, Predicción: {prediction_best}, Fitness: {abs(prediction_best - desired_output)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ffd25-76e8-4627-b3f0-a3da74ee4a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x= [97.42375995221329, 92.99244758064692, 94.66614028951585, 84.31716953093942, 64.58471808817177, 65.56549984921064, 84.65530936151131, 78.97155970760377, 41.10966437383675, 74.20977029479558, 93.49575711119695, 91.1191902578862, 75.83371697767345, 79.74900801910867, 61.43827504156183, 94.85133966251193, 97.41642359172391]\n",
    "\n",
    "\n",
    "series = TimeSeries.from_values(np.array(x).reshape(1, -1))\n",
    "x_scaled = darts_scaler.transform(series)\n",
    "prediction = regressor.predict(x_scaled.values())[0]\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016c86a-9a6f-4ca6-9edd-683da6236547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names =  [\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"cabin_in_surveyed_flight\",\n",
    "    \"haul\",\"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\",\n",
    "    \"load_factor\",\n",
    "    \"otp15_takeoff\",\n",
    "    \"NPS_weighted\",\n",
    "    \"insert_date_ci\"\n",
    "]\n",
    "\n",
    "individual = [71.84856075258804, 95.33316761178381, 80.53743268440473, 100, 76.02849620274152, 72.96724176680158, \n",
    "        97.63242229989943, 90.21300041214877, 44.49390640706396, 99.38310607594644, 85.62715830119255, 81.48347972667862, \n",
    "        69.37947451916642, 92.85334359929657, 53.1402991538944, 97.85631448138926, 88.91503365519448]\n",
    "values = ['2024-01-01', '2024-06-01', 'Economy', 'SH'] + individual + [f'{desired_output}', '2024-06-19']\n",
    "\n",
    "ytd_2024 = [\n",
    "    \"2024-01-01\",\n",
    "    \"2024-06-01\",\n",
    "    \"Economy\",\n",
    "    \"SH\",\n",
    "    71.84856075258804,\n",
    "    79.06015584914005,\n",
    "    80.29968621466156,\n",
    "    74.394075935135,\n",
    "    76.02849620274152,\n",
    "    72.96724176680158,\n",
    "    78.49867970533091,\n",
    "    43.23740683347413,\n",
    "    44.49390640706396,\n",
    "    53.73307758411367,\n",
    "    81.1828997942445,\n",
    "    78.59234695868854,\n",
    "    69.37947451916642,\n",
    "    65.98255786642703,\n",
    "    53.1402991538944,\n",
    "    87.94953821163229,\n",
    "    88.62830908016969,\n",
    "    '2024-06-01'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary\n",
    "data = {feature: value for feature, value in zip(feature_names, values)}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "ytd_data ={feature: value for feature, value in zip(feature_names, ytd_2024)}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "ytd_df = pd.DataFrame([ytd_data])\n",
    "\n",
    "# Display the DataFrame\n",
    "concatenated_df = pd.concat([ytd_df, df]).sort_index(kind='merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca237e7-39a9-4db1-873f-921433eb2ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "lightgbm.plot_importance(lgb_model, importance_type=\"gain\", figsize=(7,6), title=\"LightGBM Feature Importance (Gain)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34039d37-d4ec-4cd7-9b81-4a8381bb7f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap_explain = ShapExplainer(model=model)\n",
    "shap_explain.summary_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b27700-887a-4ae3-89ff-424badd020f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba80b6-416d-4b49-b32c-651ad126d2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsh_xlsx = augmented_dfs['Business_SH_df'][['cabin_in_surveyed_flight', 'haul', 'insert_date_ci', 'start_date', 'end_date', 'otp15_takeoff', 'out_prob_nps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f460c-d7e6-4f0e-8da1-5d7cf51f35c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsh_xlsx.to_excel('business_sh_inc_otp_08052024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eeb27a-b7d1-4349-bb3d-164f49339188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eco_xlsx = augmented_dfs['Economy_SH_df'][['cabin_in_surveyed_flight', 'haul', 'insert_date_ci', 'start_date', 'end_date', 'otp15_takeoff', 'out_prob_nps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383693f0-2dcf-4cc7-bf69-47655f24b6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eco_xlsx.to_excel('economy_sh_inc_otp_08052024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e7928-59fd-4fcb-bd85-a6a85258daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_LH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e37fa-4e03-4569-b3bf-ae958b2e29cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented_dfs['Business_LH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4926c6-9c09-418d-a645-2e2d2e821248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf89f03-608f-4384-a39f-29ef34f9229a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented_dfs['Business_SH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd0051-7c3a-4d44-839c-735fe45f3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dfs['Economy_LH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991343ed-f82f-4bab-a1d8-80e26692a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dfs['Business_LH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640c790-047b-4837-aa98-432d3eb55355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_LH_explanation=explanations['Economy_LH_df'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de925aea-d2ad-4ffa-a4e8-1d0a43b434d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explanations['Business_LH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1755ce0-0096-4b1c-9164-57d2cc30a102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chinese_LH_explanation=explanations['Economy_LH_df'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38084d39-569d-4e7b-addc-b365e74ec55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8864a-a662-462b-b662-a2e6d18924b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_uplifting_explanation(explanation2, explanation1):\n",
    "    \"\"\"\n",
    "    Create a new Explanation object representing the uplifting between two Explanation objects.\n",
    "\n",
    "    Parameters:\n",
    "        - explanation1: The first shap.Explanation object.\n",
    "        - explanation2: The second shap.Explanation object.\n",
    "\n",
    "    Returns:\n",
    "        - A new shap.Explanation object representing the uplifting.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in values, base_values, and data\n",
    "    diff_values = explanation2.values - explanation1.values\n",
    "    \n",
    "    diff_base_values = explanation1.base_values + sum(explanation1.values)\n",
    "    diff_data = explanation2.data - explanation1.data\n",
    "\n",
    "    # Create a new Explanation object with the difference values\n",
    "    diff_explanation = shap.Explanation(values=diff_values, base_values=diff_base_values, data=diff_data,\n",
    "                                        feature_names=explanation1.feature_names)\n",
    "\n",
    "    return diff_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64132109-7430-4729-90f1-dd858dbf91a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "march_diff_explanation = create_uplifting_explanation(chinese_LH_explanation, overall_LH_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b6d2c-cb54-423c-b5a7-f780bf93070e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.plots.waterfall(overall_LH_explanation, max_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c6973-9249-4d3c-89d9-c0ff52c1d5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented_dfs['Business_LH_df'].to_excel('shaps_for_march_and_april_Business_LH_comparison.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fe3a7-5eaf-4d3b-af2f-5796e429cc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # Reconstruir el DataFrame original\n",
    "df = pd.concat(augmented_dfs.values())\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0383ba-8e94-4a24-8864-c46fbaa495ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('weekly_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f49ce-33ea-483b-89a6-0d32e3786b39",
   "metadata": {},
   "source": [
    "## Importance representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188647b2-989e-433d-9218-de0d0ce565d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('daily_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f23ce-d07f-4c3d-955c-77b801a00db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed5cde-edc4-4e0d-b0f1-6078b1122dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Suponemos que df es tu DataFrame y ya está definido y cargado con los datos necesarios.\n",
    "haul = 'SH'\n",
    "cabin = 'Economy'\n",
    "variable = 'otp15_takeoff'\n",
    "real_target = 'NPS_weighted'\n",
    "predicted_target = 'out_prob_nps'\n",
    "\n",
    "# Define la función para remover outliers\n",
    "def remove_outliers(data, threshold=1.5):\n",
    "    q25 = np.percentile(data, 25)\n",
    "    q75 = np.percentile(data, 75)\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - threshold * iqr\n",
    "    upper_bound = q75 + threshold * iqr\n",
    "    return (data >= lower_bound) & (data <= upper_bound)\n",
    "\n",
    "# Filtrado de datos\n",
    "filtered_data = df[(df['haul'] == haul) & (df['cabin_in_surveyed_flight'] == cabin)]\n",
    "x = filtered_data[variable].to_numpy().reshape(-1, 1)  # Características\n",
    "y_real = filtered_data[real_target].to_numpy()  # Valores reales\n",
    "y_pred = filtered_data[predicted_target].to_numpy()  # Valores predichos\n",
    "\n",
    "# Identificar índices de datos válidos sin outliers para x y y_real\n",
    "valid_x = remove_outliers(x.flatten())\n",
    "valid_y_real = remove_outliers(y_real)\n",
    "\n",
    "# Filtrar x, y_real, y_pred usando índices válidos\n",
    "valid_indices = valid_x & valid_y_real\n",
    "x_clean = x[valid_indices].reshape(-1, 1)\n",
    "y_real_clean = y_real[valid_indices]\n",
    "y_pred_clean = y_pred[valid_indices]\n",
    "\n",
    "# Ajustar el modelo de regresión lineal con datos limpios\n",
    "model = LinearRegression()\n",
    "model.fit(x_clean, y_real_clean)\n",
    "\n",
    "# Generar valores para la línea de regresión\n",
    "x_fit = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)\n",
    "y_fit = model.predict(x_fit)\n",
    "\n",
    "# Crear el gráfico de dispersión con la línea de regresión\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x_clean, y_real_clean, color='black', alpha=0.6, edgecolors='w', linewidths=0.5, label='Real NPS')\n",
    "plt.title(f'Impact of {variable} on NPS - Real vs. Predicted with Regression')\n",
    "plt.xlabel(f'{variable} Values')\n",
    "plt.ylabel('NPS Values')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5103b-8ac4-4e63-a1ed-17cfdaf62375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crear el gráfico de dispersión con la línea de regresión\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x_clean, y_real_clean, color='blue', alpha=0.6, edgecolors='w', linewidths=0.5, label='Real NPS')\n",
    "plt.plot(x_fit, y_fit, color='green', linewidth=2, label='Regression Line')  # Añadir la línea de regresión\n",
    "plt.title(f'Impact of {variable} on NPS - Real vs. Predicted with Regression')\n",
    "plt.xlabel(f'{variable} Values')\n",
    "plt.ylabel('NPS Values')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Mover la anotación a la esquina inferior izquierda\n",
    "plt.annotate(f'Slope: {model.coef_[0]:.2f}\\nIntercept: {model.intercept_:.2f}\\nR² Score: {model.score(x_clean, y_real_clean):.2f}', \n",
    "             xy=(0.05, 0.05), xycoords='axes fraction', verticalalignment='bottom', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='green', facecolor='white'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b03c50-af6a-4d8f-adb0-fe786b6015e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crear el gráfico de dispersión con la línea de regresión\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x_clean, y_real_clean, color='blue', alpha=0.6, edgecolors='w', linewidths=0.5, label='Real NPS')\n",
    "plt.scatter(x_clean, y_pred_clean, color='red', alpha=0.6, edgecolors='w', linewidths=0.5, label='Predicted NPS')\n",
    "plt.plot(x_fit, y_fit, color='green', linewidth=2, label='Regression Line')  # Añadir la línea de regresión\n",
    "plt.title(f'Impact of {variable} on NPS - Real vs. Predicted with Regression')\n",
    "plt.xlabel(f'{variable} Values')\n",
    "plt.ylabel('NPS Values')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Mover la anotación a la esquina inferior izquierda\n",
    "plt.annotate(f'Slope: {model.coef_[0]:.2f}\\nIntercept: {model.intercept_:.2f}\\nR² Score: {model.score(x_clean, y_real_clean):.2f}', \n",
    "             xy=(0.05, 0.05), xycoords='axes fraction', verticalalignment='bottom', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='green', facecolor='white'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42eb2e0-3e6e-4667-ad33-87195c6e6dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# variable = 'ifl_100_cabin_crew_satisfaction'\n",
    "variable = 'otp15_takeoff'\n",
    "target = f'{variable}_nps'\n",
    "# target = 'out_prob_nps'\n",
    "# target= 'NPS_weighted'\n",
    "\n",
    "\n",
    "# Filter the data\n",
    "filtered_data = df[(df['haul'] == haul) & (df['cabin_in_surveyed_flight'] == cabin)]\n",
    "x = filtered_data[variable].to_numpy().reshape(-1, 1)  # Features\n",
    "y = filtered_data[target].to_numpy()  # Target\n",
    "\n",
    "# Define a function to remove outliers\n",
    "def remove_outliers(data, threshold=1.5):\n",
    "    q25 = np.percentile(data, 10)\n",
    "    q75 = np.percentile(data, 90)\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - threshold * iqr\n",
    "    upper_bound = q75 + threshold * iqr\n",
    "    return (data >= lower_bound) & (data <= upper_bound)\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and x, y are already defined as numpy arrays.\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data to get the relevant subset\n",
    "filtered_data = df[(df['haul'] == haul) & (df['cabin_in_surveyed_flight'] == cabin)]\n",
    "x = filtered_data[variable].values.reshape(-1, 1)  # Features as numpy array\n",
    "y = filtered_data[target].values            # Target as numpy array\n",
    "\n",
    "# Identify non-outlier indices for both x and y\n",
    "valid_x = remove_outliers(x.flatten())  # flatten x to 1D for consistency with y\n",
    "valid_y = remove_outliers(y)\n",
    "\n",
    "# Get common indices where both x and y are non-outliers\n",
    "valid_indices = valid_x & valid_y\n",
    "\n",
    "# Filter both x and y using the valid_indices\n",
    "x_clean = x[valid_indices].reshape(-1, 1)\n",
    "y_clean = y[valid_indices]\n",
    "\n",
    "# Fit the linear regression model with cleaned data\n",
    "model_clean = LinearRegression()\n",
    "model_clean.fit(x_clean, y_clean)\n",
    "\n",
    "# Get model parameters\n",
    "slope_clean = model_clean.coef_[0]\n",
    "intercept_clean = model_clean.intercept_\n",
    "r2_score_clean = model_clean.score(x_clean, y_clean)\n",
    "\n",
    "# Generate values for the regression line\n",
    "x_fit = np.linspace(x_clean.min(), x_clean.max(), 100).reshape(-1, 1)\n",
    "y_fit = model_clean.predict(x_fit)\n",
    "\n",
    "# Create the scatter plot with regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_clean, y_clean, alpha=0.6, edgecolors='w', linewidths=0.5)\n",
    "plt.plot(x_fit, y_fit, color='red', linewidth=2)  # Add the regression line\n",
    "plt.title(f'{variable} vs {target}')\n",
    "plt.xlabel(f'{variable} Actual Values')\n",
    "plt.ylabel(f'{target}')\n",
    "plt.grid(True)\n",
    "plt.annotate(f'Slope: {slope_clean:.2f}\\nIntercept: {intercept_clean:.2f}\\nR² Score: {r2_score_clean:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "             verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='red', facecolor='white'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ce880-4bbc-494e-8d2b-27ec3bf7e7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebd0e-6ad1-477c-95c8-63d2df5a750c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Generate synthetic data with a non-linear relationship\n",
    "np.random.seed(0)\n",
    "X1 = np.random.uniform(-3, 3, 100)\n",
    "X2 = np.random.uniform(-3, 3, 100)\n",
    "Y = np.sin(X1) + np.cos(X2) + np.random.normal(0, 0.1, 100)  # Non-linear relationship\n",
    "\n",
    "# Fit a non-linear model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(np.column_stack((X1, X2)), Y)\n",
    "\n",
    "# Predictions for visualization\n",
    "x1_range = np.linspace(-3, 3, 100)\n",
    "x2_range = np.linspace(-3, 3, 100)\n",
    "X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "Y_pred = model.predict(np.c_[X1_grid.ravel(), X2_grid.ravel()]).reshape(X1_grid.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.contourf(X1_grid, X2_grid, Y_pred, levels=30, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Predicted Y from Non-Linear Model')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef8ed2-1c73-4b8b-8c0b-e1501d4f9ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_pred = df[(df['insert_date_ci']=='2023-11-21')&(pd.to_datetime(df['end_date']).dt.year>2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7648d1-e4e8-4830-a43d-a9e883cdc8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_pred[['start_date', 'end_date','cabin_in_surveyed_flight', 'haul', 'NPS_weighted', 'out_prob_nps', 'out_prob_base']]\n",
    "targets_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612263ab-9405-42aa-a59d-dcd3928aeaa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= targets_pred.copy()\n",
    "# Calcular la diferencia entre NPS_weighted y out_prob_nps\n",
    "df['difference'] = df['NPS_weighted'] - df['out_prob_nps']\n",
    "\n",
    "# Obtener las columnas de SHAP que terminan en \"_nps\"\n",
    "shap_columns = [col for col in df.columns if col.endswith('_nps') and col != 'out_prob_nps']\n",
    "\n",
    "# Función para ajustar los valores de SHAP\n",
    "def adjust_shap_values(row):\n",
    "    difference = row['difference']\n",
    "    if difference > 0:\n",
    "        # Filtrar los valores SHAP positivos\n",
    "        positive_shaps = [col for col in shap_columns if row[col] > 0]\n",
    "        if positive_shaps:\n",
    "            adjustment = difference / len(positive_shaps)\n",
    "            for col in positive_shaps:\n",
    "                row[col] += adjustment\n",
    "    elif difference < 0:\n",
    "        # Filtrar los valores SHAP negativos\n",
    "        negative_shaps = [col for col in shap_columns if row[col] < 0]\n",
    "        if negative_shaps:\n",
    "            adjustment = difference / len(negative_shaps)\n",
    "            for col in negative_shaps:\n",
    "                row[col] += adjustment\n",
    "    return row\n",
    "\n",
    "# Aplicar la función de ajuste a cada fila del dataframe\n",
    "df = df.apply(adjust_shap_values, axis=1)\n",
    "\n",
    "# Actualizar la columna out_prob_nps con la suma de los nuevos valores SHAP y el valor base\n",
    "df['out_prob_nps'] = df[shap_columns].sum(axis=1) + df['out_prob_base']\n",
    "\n",
    "# Eliminar la columna de diferencia ya que no es necesaria\n",
    "df.drop(columns=['difference'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed819b-176a-48bb-816c-0cbcf37ccd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_and_adjust_shap(df, shap_columns, desired_nps, base_output_prob, clamp_min, clamp_max):\n",
    "    # Calculate the target SHAP sum, which is the desired NPS minus the base model output probability\n",
    "    df['target_shap_sum'] = desired_nps - base_output_prob\n",
    "\n",
    "    # Proceed to normalize and clamp SHAP values\n",
    "    df = normalize_clamp_shap(df, shap_columns, df['target_shap_sum'], clamp_min, clamp_max)\n",
    "\n",
    "    # Update 'out_prob_nps' with the new SHAP values sum and the base output probability\n",
    "    df['out_prob_nps'] = df['out_prob_base'] + df[shap_columns].sum(axis=1)\n",
    "\n",
    "    # Check if the new 'out_prob_nps' matches 'NPS_weighted'\n",
    "    df['is_correct_nps'] = np.isclose(df['out_prob_nps'], df['NPS_weighted'], atol=1e-5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_clamp_shap(df, shap_columns, target_shap_sum, clamp_min, clamp_max):\n",
    "    # Calculate the initial sum of SHAP values\n",
    "    current_shap_total = df[shap_columns].sum(axis=1)\n",
    "\n",
    "    # Normalize SHAP values to match the target prediction error\n",
    "    normalized_shap = df[shap_columns].div(current_shap_total, axis=0).mul(target_shap_sum, axis=0)\n",
    "\n",
    "    # Apply proportional scaling to ensure all values are within bounds\n",
    "    scaling_factor = np.maximum(np.abs(normalized_shap / clamp_max), np.abs(normalized_shap / clamp_min))\n",
    "    scaling_factor = scaling_factor.max(axis=1)\n",
    "\n",
    "    # Adjust scaling factor to avoid division by zero and ensure it's at least 1\n",
    "    scaling_factor[scaling_factor < 1] = 1\n",
    "\n",
    "    # Apply scaling\n",
    "    adjusted_shap = normalized_shap.div(scaling_factor, axis=0)\n",
    "\n",
    "    # Assign adjusted SHAP values back, ensuring they stay within bounds\n",
    "    adjusted_shap = adjusted_shap.clip(lower=clamp_min, upper=clamp_max)\n",
    "    df.loc[:, shap_columns] = adjusted_shap\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "clamp_min, clamp_max = -5, 5  # Define bounds\n",
    "desired_nps = targets_pred['NPS_weighted']  # Assuming this is your desired NPS\n",
    "base_output_prob = targets_pred['out_prob_base']  # Assuming this is your base output probability\n",
    "\n",
    "adjusted_df = prepare_and_adjust_shap(targets_pred.copy(), shap_columns, desired_nps, base_output_prob, clamp_min, clamp_max)\n",
    "print(adjusted_df[['out_prob_nps', 'NPS_weighted', 'is_correct_nps']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c7b95-519e-46b0-bcfb-0706a5f36a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac2146-509a-408f-ac2c-5a35c59d4585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_and_adjust_shap(df, shap_columns, desired_nps, base_output_prob, clamp_min, clamp_max):\n",
    "    # Calculate the target SHAP sum, which is the desired NPS minus the base model output probability\n",
    "    df['target_shap_sum'] = desired_nps - base_output_prob\n",
    "\n",
    "    # Normalize and clamp SHAP values\n",
    "    df = normalize_clamp_shap(df, shap_columns, df['target_shap_sum'], clamp_min, clamp_max)\n",
    "\n",
    "    # Update 'out_prob_nps' with the new SHAP values sum and the base output probability\n",
    "    df['out_prob_nps'] = df['out_prob_base'] + df[shap_columns].sum(axis=1)\n",
    "\n",
    "    # Apply a final normalization if 'out_prob_nps' does not match 'NPS_weighted'\n",
    "    df = final_normalization(df, shap_columns, desired_nps)\n",
    "\n",
    "    # Check if the new 'out_prob_nps' matches 'NPS_weighted'\n",
    "    df['is_correct_nps'] = np.isclose(df['out_prob_nps'], df['NPS_weighted'], atol=1e-5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_clamp_shap(df, shap_columns, target_shap_sum, clamp_min, clamp_max):\n",
    "    current_shap_total = df[shap_columns].sum(axis=1)\n",
    "    normalized_shap = df[shap_columns].div(current_shap_total, axis=0).mul(target_shap_sum, axis=0)\n",
    "    clamped_shap = normalized_shap.clip(lower=clamp_min, upper=clamp_max)\n",
    "    df[shap_columns] = clamped_shap\n",
    "    return df\n",
    "\n",
    "def final_normalization(df, shap_columns, desired_nps):\n",
    "    # Calculate the total contribution needed from SHAP values to meet the desired NPS\n",
    "    total_needed_shap = desired_nps - df['out_prob_base']\n",
    "    current_shap_sum = df[shap_columns].sum(axis=1)\n",
    "\n",
    "    # Determine the factor by which to adjust the SHAP values\n",
    "    normalization_factor = total_needed_shap / current_shap_sum\n",
    "\n",
    "    # Adjust SHAP values\n",
    "    df[shap_columns] = df[shap_columns].mul(normalization_factor, axis=0)\n",
    "    df['out_prob_nps'] = df['out_prob_base'] + df[shap_columns].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "clamp_min, clamp_max = -5, 5  # Define bounds\n",
    "desired_nps = targets_pred['NPS_weighted']  # Assuming this is your desired NPS\n",
    "base_output_prob = targets_pred['out_prob_base']  # Assuming this is your base output probability\n",
    "\n",
    "adjusted_df = prepare_and_adjust_shap(targets_pred.copy(), shap_columns, desired_nps, base_output_prob, clamp_min, clamp_max)\n",
    "print(adjusted_df[['out_prob_nps', 'NPS_weighted', 'is_correct_nps']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170b5d0-fb7e-4125-b826-daeb4bf9c9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Business') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-01-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'loy_200_loyalty_programme_satisfaction_nps'] = -3.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812812e-6aae-403b-8c4b-977042e8bc60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Business') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-01-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'img_310_ease_contact_phone_satisfaction_nps'] = -1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e92771-c4af-4f63-824e-645f27ab60b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-08-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'ifl_100_cabin_crew_satisfaction_nps'] = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b2a45-839a-4596-9a85-2afafb100f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-08-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'ifl_400_food_drink_satisfaction_nps'] = 4.332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec08f7-935e-411c-8c26-e6e768e08b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-08-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'pfl_100_checkin_satisfaction_nps'] = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e5663-a322-4422-8876-b295c0a76178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'otp15_takeoff_nps'] = 4.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c050cf7-c0da-4492-be0b-c4f33a730798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'load_factor_nps'] = 1.863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0632f3-ecce-4598-bc34-fcd375b0d858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'img_310_ease_contact_phone_satisfaction_nps'] = 2.085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900c12b-5dfe-44d9-b04a-43a0fc4a6e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'ifl_400_food_drink_satisfaction_nps'] = -4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765c463-94ea-4495-bd14-6a6fce086e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'pfl_100_checkin_satisfaction_nps'] = 4.092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09581fea-e996-4e19-8f1f-466f93b1857e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'ifl_100_cabin_crew_satisfaction_nps'] = -2.592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6cfce-0a94-428a-b957-937747261fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on the conditions\n",
    "condition = (\n",
    "    (adjusted_df['cabin_in_surveyed_flight'] == 'Premium Economy') & \n",
    "    (adjusted_df['haul'] == 'LH') & \n",
    "    (adjusted_df['start_date'] == '2024-10-01')\n",
    ")\n",
    "\n",
    "# Set the 'loy_200_loyalty_programme_satisfaction_nps' column value to -3.6 for these rows\n",
    "adjusted_df.loc[condition, 'ifl_200_flight_crew_annoucements_satisfaction_nps'] = -0.254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75008a-b797-4e4e-b2b2-004a1842fae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_final(df, shap_columns, desired_nps, base_output_prob):\n",
    "    # Update 'out_prob_nps' with the new SHAP values sum and the base output probability\n",
    "    df['out_prob_nps'] = df['out_prob_base'] + df[shap_columns].sum(axis=1)\n",
    "\n",
    "    # Check if the new 'out_prob_nps' matches 'NPS_weighted'\n",
    "    df['is_correct_nps'] = np.isclose(df['out_prob_nps'], df['NPS_weighted'], atol=1e-5)\n",
    "\n",
    "    return df\n",
    "adjusted_df = check_final(adjusted_df.copy(), shap_columns, desired_nps, base_output_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34f13e-bea4-4540-a1c8-f8ea559cb2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(adjusted_df[['out_prob_nps', 'NPS_weighted', 'is_correct_nps']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26998199-300d-46a0-829d-1d4ad56326a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = final_normalization(adjusted_df, shap_columns, desired_nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42877072-c6ba-4fcc-ac63-d96c0bd85ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.isclose(df['out_prob_nps'], df['NPS_weighted'], atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51228d6d-7021-45fc-9550-134e8b7e3c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606712c-b8da-4d8a-9333-edde8d8c5375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_shap_sum_column(df, shap_columns):\n",
    "    # Calculate the sum of SHAP values across the specified columns for each row\n",
    "    df['sum_adjusted_shaps'] = df[shap_columns].sum(axis=1) + df['out_prob_base']\n",
    "    return df\n",
    "\n",
    "# Apply the function to add the sum column to the adjusted DataFrame\n",
    "adjusted_df = add_shap_sum_column(adjusted_df, shap_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89168858-5fbd-44eb-81e9-88adec3412b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_df[['start_date', 'end_date','cabin_in_surveyed_flight', 'haul', 'NPS_weighted', 'out_prob_nps','sum_adjusted_shaps', 'out_prob_base']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723dfcfe-bc54-416e-b808-c52985a1f8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd5728-b9e7-442c-93e9-0cf50aebe8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a636d-3173-4077-a0d7-6ed64734cd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Identify all Shapley columns, excluding 'out_prob_nps'\n",
    "shap_columns = [col for col in targets_pred.columns if col.endswith('_nps') and col != 'out_prob_nps']\n",
    "print(shap_columns)\n",
    "\n",
    "# Step 2: Calculate the current total Shapley values per row\n",
    "current_shap_total = targets_pred[shap_columns].sum(axis=1)\n",
    "\n",
    "# Step 3: Calculate the target Shapley sum (NPS_weighted - out_prob_base)\n",
    "target_shap_sum = targets_pred['NPS_weighted'] - targets_pred['out_prob_base']\n",
    "\n",
    "# Step 4: Calculate the adjustment ratio\n",
    "adjustment_ratio = target_shap_sum / current_shap_total\n",
    "\n",
    "# Step 5: Adjust Shapley values using the adjustment ratio\n",
    "for col in shap_columns:\n",
    "    targets_pred[col] *= adjustment_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9df564-d5a3-40a7-a5cb-070caf577237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e2181-a604-48f6-a0c1-51ce2c435c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_excel('final_shaps_for_targets.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6da2a-d256-46cd-b161-1760589c07c1",
   "metadata": {},
   "source": [
    "# Debug concatenated targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5cf6c4-19af-40fe-8cf5-8ea35068a5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets_df= pd.read_csv('corrected_nps_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6446bb0-949e-40a2-a4d1-0d3ae99c272f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_historic = pd.read_csv('historic_predictions_q1 (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41a3be-b696-4749-b5f0-dfddb2b5e837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([df_historic,targets_df.drop(columns='NPS_weighted')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b1cbb-1556-4595-ab6d-f4881b9300c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1ad48-7744-4b3f-b1b7-56339f0266f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
